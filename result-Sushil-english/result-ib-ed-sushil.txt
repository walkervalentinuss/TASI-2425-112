I0000 00:00:1746660816.576519      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5
I0000 00:00:1746660816.577263      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5
config.json: 100%
 1.54k/1.54k [00:00<00:00, 167kB/s]
tf_model.h5: 100%
 63.1M/63.1M [00:00<00:00, 171MB/s]
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Running experiment with batch_size=16, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.7290 | Val Loss: 0.6993 | Time: 43.19s
Train Loss: 0.7362 | Val Loss: 0.6928 | Time: 82.91s
Train Loss: 0.7268 | Val Loss: 0.6868 | Time: 122.99s
Train Loss: 0.7269 | Val Loss: 0.6927 | Time: 162.28s
Train Loss: 0.7185 | Val Loss: 0.6958 | Time: 201.53s
Train Loss: 0.7137 | Val Loss: 0.6987 | Time: 240.62s
Early stopping...

Epoch 2/2
Train Loss: 0.7338 | Val Loss: 0.7033 | Time: 39.57s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/3
Train Loss: 0.7035 | Val Loss: 0.7069 | Time: 39.54s
Train Loss: 0.7142 | Val Loss: 0.7065 | Time: 78.86s
Train Loss: 0.6976 | Val Loss: 0.7074 | Time: 117.97s
Train Loss: 0.6695 | Val Loss: 0.7172 | Time: 156.97s
Train Loss: 0.6707 | Val Loss: 0.7066 | Time: 195.99s
Early stopping...

Epoch 2/3
Train Loss: 0.7746 | Val Loss: 0.6982 | Time: 39.09s
Train Loss: 0.7120 | Val Loss: 0.7068 | Time: 78.27s
Train Loss: 0.7063 | Val Loss: 0.6975 | Time: 117.04s
Train Loss: 0.7401 | Val Loss: 0.6967 | Time: 156.03s
Train Loss: 0.7238 | Val Loss: 0.6994 | Time: 195.23s
Train Loss: 0.7331 | Val Loss: 0.6962 | Time: 233.85s
Train Loss: 0.7244 | Val Loss: 0.6951 | Time: 273.47s
Train Loss: 0.7103 | Val Loss: 0.6939 | Time: 312.69s
Train Loss: 0.7005 | Val Loss: 0.6956 | Time: 351.32s
Train Loss: 0.7006 | Val Loss: 0.6976 | Time: 389.43s
Train Loss: 0.7071 | Val Loss: 0.7000 | Time: 427.56s
Early stopping...

Epoch 3/3
Train Loss: 0.7250 | Val Loss: 0.6995 | Time: 38.09s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/4
Train Loss: 0.7912 | Val Loss: 0.6967 | Time: 37.99s
Train Loss: 0.7362 | Val Loss: 0.7027 | Time: 75.79s
Train Loss: 0.7388 | Val Loss: 0.7061 | Time: 113.68s
Train Loss: 0.7366 | Val Loss: 0.7100 | Time: 151.59s
Early stopping...

Epoch 2/4
Train Loss: 0.6570 | Val Loss: 0.7090 | Time: 38.03s
Early stopping...

Epoch 3/4
Train Loss: 0.7768 | Val Loss: 0.7007 | Time: 38.14s
Early stopping...

Epoch 4/4
Train Loss: 0.7023 | Val Loss: 0.6923 | Time: 37.99s
Train Loss: 0.7002 | Val Loss: 0.6963 | Time: 76.33s
Train Loss: 0.7191 | Val Loss: 0.6979 | Time: 114.78s
Train Loss: 0.7073 | Val Loss: 0.7043 | Time: 153.01s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/2
Train Loss: 0.6755 | Val Loss: 0.6901 | Time: 38.05s
Train Loss: 0.7110 | Val Loss: 0.6889 | Time: 75.88s
Train Loss: 0.7004 | Val Loss: 0.6853 | Time: 113.81s
Train Loss: 0.7105 | Val Loss: 0.6894 | Time: 151.75s
Train Loss: 0.7213 | Val Loss: 0.6885 | Time: 189.95s
Train Loss: 0.7152 | Val Loss: 0.6917 | Time: 227.89s
Early stopping...

Epoch 2/2
Train Loss: 0.6881 | Val Loss: 0.6894 | Time: 37.80s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/3
Train Loss: 0.6388 | Val Loss: 0.6955 | Time: 37.84s
Train Loss: 0.6605 | Val Loss: 0.6926 | Time: 75.85s
Train Loss: 0.6778 | Val Loss: 0.6937 | Time: 114.53s
Train Loss: 0.6882 | Val Loss: 0.6957 | Time: 153.27s
Train Loss: 0.6853 | Val Loss: 0.6934 | Time: 192.04s
Early stopping...

Epoch 2/3
Train Loss: 0.7208 | Val Loss: 0.6892 | Time: 38.90s
Train Loss: 0.6767 | Val Loss: 0.6849 | Time: 78.33s
Train Loss: 0.6812 | Val Loss: 0.6835 | Time: 117.07s
Train Loss: 0.6878 | Val Loss: 0.6848 | Time: 155.89s
Train Loss: 0.6906 | Val Loss: 0.6844 | Time: 194.43s
Train Loss: 0.6940 | Val Loss: 0.6807 | Time: 233.04s
Train Loss: 0.6839 | Val Loss: 0.6799 | Time: 271.84s
Train Loss: 0.6719 | Val Loss: 0.6793 | Time: 310.38s
Train Loss: 0.6842 | Val Loss: 0.6815 | Time: 349.33s
Train Loss: 0.6857 | Val Loss: 0.6820 | Time: 388.31s
Train Loss: 0.6890 | Val Loss: 0.6828 | Time: 427.30s
Early stopping...

Epoch 3/3
Train Loss: 0.6903 | Val Loss: 0.6816 | Time: 39.61s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/4
Train Loss: 0.6995 | Val Loss: 0.6933 | Time: 39.43s
Train Loss: 0.6853 | Val Loss: 0.6886 | Time: 78.59s
Train Loss: 0.6947 | Val Loss: 0.6865 | Time: 117.74s
Train Loss: 0.6870 | Val Loss: 0.6854 | Time: 156.92s
Train Loss: 0.6795 | Val Loss: 0.6836 | Time: 195.67s
Train Loss: 0.6639 | Val Loss: 0.6843 | Time: 234.59s
Train Loss: 0.6604 | Val Loss: 0.6832 | Time: 273.46s
Train Loss: 0.6551 | Val Loss: 0.6830 | Time: 312.44s
Train Loss: 0.6598 | Val Loss: 0.6823 | Time: 351.28s
Train Loss: 0.6657 | Val Loss: 0.6811 | Time: 390.41s
Train Loss: 0.6575 | Val Loss: 0.6805 | Time: 429.37s
Train Loss: 0.6671 | Val Loss: 0.6797 | Time: 468.08s
Train Loss: 0.6688 | Val Loss: 0.6778 | Time: 507.06s
Train Loss: 0.6662 | Val Loss: 0.6763 | Time: 545.73s
Train Loss: 0.6651 | Val Loss: 0.6760 | Time: 584.62s
Train Loss: 0.6689 | Val Loss: 0.6763 | Time: 623.33s
Train Loss: 0.6727 | Val Loss: 0.6749 | Time: 662.34s
Train Loss: 0.6736 | Val Loss: 0.6723 | Time: 701.25s
Train Loss: 0.6722 | Val Loss: 0.6702 | Time: 740.41s
Train Loss: 0.6703 | Val Loss: 0.6695 | Time: 779.44s
Train Loss: 0.6735 | Val Loss: 0.6694 | Time: 818.16s
Train Loss: 0.6715 | Val Loss: 0.6714 | Time: 857.24s
Train Loss: 0.6740 | Val Loss: 0.6745 | Time: 896.22s
Early stopping...

Epoch 2/4
Train Loss: 0.7006 | Val Loss: 0.6782 | Time: 39.28s
Early stopping...

Epoch 3/4
Train Loss: 0.6841 | Val Loss: 0.6797 | Time: 38.86s
Early stopping...

Epoch 4/4
Train Loss: 0.7335 | Val Loss: 0.6827 | Time: 38.37s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/2
Train Loss: 0.7097 | Val Loss: 0.6898 | Time: 38.70s
Train Loss: 0.7188 | Val Loss: 0.6880 | Time: 77.56s
Train Loss: 0.7056 | Val Loss: 0.6897 | Time: 116.17s
Train Loss: 0.6986 | Val Loss: 0.6926 | Time: 154.77s
Train Loss: 0.6925 | Val Loss: 0.6986 | Time: 193.58s
Early stopping...

Epoch 2/2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6966 | Val Loss: 0.6962 | Time: 38.87s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.6969 | Val Loss: 0.6909 | Time: 38.51s
Train Loss: 0.7074 | Val Loss: 0.6907 | Time: 77.17s
Train Loss: 0.6905 | Val Loss: 0.6942 | Time: 116.28s
Train Loss: 0.6779 | Val Loss: 0.6963 | Time: 155.12s
Train Loss: 0.6812 | Val Loss: 0.7010 | Time: 193.94s
Early stopping...

Epoch 2/3
Train Loss: 0.7730 | Val Loss: 0.6993 | Time: 39.23s
Early stopping...

Epoch 3/3
Train Loss: 0.7482 | Val Loss: 0.6950 | Time: 38.75s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/4
Train Loss: 0.6364 | Val Loss: 0.7194 | Time: 38.62s
Train Loss: 0.7118 | Val Loss: 0.7148 | Time: 77.42s
Train Loss: 0.7366 | Val Loss: 0.7085 | Time: 115.83s
Train Loss: 0.7311 | Val Loss: 0.7011 | Time: 154.50s
Train Loss: 0.7278 | Val Loss: 0.6942 | Time: 192.99s
Train Loss: 0.7306 | Val Loss: 0.6897 | Time: 231.44s
Train Loss: 0.7160 | Val Loss: 0.6878 | Time: 270.16s
Train Loss: 0.7193 | Val Loss: 0.6839 | Time: 308.64s
Train Loss: 0.7261 | Val Loss: 0.6814 | Time: 347.27s
Train Loss: 0.7270 | Val Loss: 0.6797 | Time: 385.63s
Train Loss: 0.7266 | Val Loss: 0.6806 | Time: 425.04s
Train Loss: 0.7216 | Val Loss: 0.6813 | Time: 463.93s
Train Loss: 0.7180 | Val Loss: 0.6820 | Time: 502.50s
Early stopping...

Epoch 2/4
Train Loss: 0.6910 | Val Loss: 0.6820 | Time: 38.76s
Early stopping...

Epoch 3/4
Train Loss: 0.6698 | Val Loss: 0.6821 | Time: 38.88s
Early stopping...

Epoch 4/4
Train Loss: 0.6922 | Val Loss: 0.6813 | Time: 38.81s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/2
Train Loss: 0.6361 | Val Loss: 0.7217 | Time: 32.26s
Train Loss: 0.6884 | Val Loss: 0.7242 | Time: 64.35s
Train Loss: 0.6846 | Val Loss: 0.7045 | Time: 96.54s
Train Loss: 0.6892 | Val Loss: 0.6941 | Time: 128.71s
Train Loss: 0.6824 | Val Loss: 0.6891 | Time: 160.53s
Train Loss: 0.6789 | Val Loss: 0.6830 | Time: 192.61s
Train Loss: 0.6795 | Val Loss: 0.6830 | Time: 224.65s
Train Loss: 0.6752 | Val Loss: 0.6824 | Time: 256.52s
Train Loss: 0.6774 | Val Loss: 0.6812 | Time: 288.38s
Train Loss: 0.6781 | Val Loss: 0.6825 | Time: 320.27s
Train Loss: 0.6757 | Val Loss: 0.6856 | Time: 352.32s
Train Loss: 0.6823 | Val Loss: 0.6849 | Time: 384.47s
Early stopping...

Epoch 2/2
Train Loss: 0.6577 | Val Loss: 0.6860 | Time: 32.18s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/3
Train Loss: 0.6998 | Val Loss: 0.6918 | Time: 32.17s
Train Loss: 0.6889 | Val Loss: 0.6878 | Time: 64.19s
Train Loss: 0.7107 | Val Loss: 0.6856 | Time: 96.28s
Train Loss: 0.7028 | Val Loss: 0.6915 | Time: 128.26s
Train Loss: 0.7075 | Val Loss: 0.6937 | Time: 160.22s
Train Loss: 0.7032 | Val Loss: 0.6933 | Time: 192.20s
Early stopping...

Epoch 2/3
Train Loss: 0.6499 | Val Loss: 0.6886 | Time: 31.96s
Early stopping...

Epoch 3/3
Train Loss: 0.6740 | Val Loss: 0.6866 | Time: 32.01s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/4
Train Loss: 0.7262 | Val Loss: 0.6976 | Time: 32.06s
Train Loss: 0.7074 | Val Loss: 0.7072 | Time: 64.01s
Train Loss: 0.7208 | Val Loss: 0.7004 | Time: 95.93s
Train Loss: 0.7146 | Val Loss: 0.6954 | Time: 127.80s
Train Loss: 0.7086 | Val Loss: 0.6928 | Time: 159.68s
Train Loss: 0.7018 | Val Loss: 0.6945 | Time: 191.60s
Train Loss: 0.6998 | Val Loss: 0.6961 | Time: 223.48s
Train Loss: 0.7088 | Val Loss: 0.6927 | Time: 255.24s
Early stopping...

Epoch 2/4
Train Loss: 0.6458 | Val Loss: 0.6929 | Time: 31.95s
Early stopping...

Epoch 3/4
Train Loss: 0.6748 | Val Loss: 0.7063 | Time: 32.20s
Early stopping...

Epoch 4/4
Train Loss: 0.6828 | Val Loss: 0.7033 | Time: 32.11s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/2
Train Loss: 0.7140 | Val Loss: 0.6988 | Time: 32.21s
Train Loss: 0.6890 | Val Loss: 0.6962 | Time: 64.31s
Train Loss: 0.6934 | Val Loss: 0.6987 | Time: 96.34s
Train Loss: 0.6909 | Val Loss: 0.6940 | Time: 128.52s
Train Loss: 0.6974 | Val Loss: 0.6873 | Time: 160.65s
Train Loss: 0.6957 | Val Loss: 0.6850 | Time: 192.82s
Train Loss: 0.6935 | Val Loss: 0.6834 | Time: 225.00s
Train Loss: 0.6903 | Val Loss: 0.6848 | Time: 257.21s
Train Loss: 0.6924 | Val Loss: 0.6829 | Time: 289.43s
Train Loss: 0.6900 | Val Loss: 0.6823 | Time: 321.66s
Train Loss: 0.6900 | Val Loss: 0.6816 | Time: 353.90s
Train Loss: 0.6895 | Val Loss: 0.6809 | Time: 386.09s
Train Loss: 0.6890 | Val Loss: 0.6766 | Time: 418.27s
Train Loss: 0.6868 | Val Loss: 0.6718 | Time: 450.46s
Train Loss: 0.6854 | Val Loss: 0.6698 | Time: 482.64s
Train Loss: 0.6855 | Val Loss: 0.6707 | Time: 514.81s
Train Loss: 0.6820 | Val Loss: 0.6750 | Time: 546.97s
Train Loss: 0.6833 | Val Loss: 0.6780 | Time: 579.12s
Early stopping...

Epoch 2/2
Train Loss: 0.6475 | Val Loss: 0.6758 | Time: 32.22s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/3
Train Loss: 0.7307 | Val Loss: 0.6959 | Time: 32.30s
Train Loss: 0.7044 | Val Loss: 0.6956 | Time: 64.46s
Train Loss: 0.7115 | Val Loss: 0.6908 | Time: 96.66s
Train Loss: 0.6929 | Val Loss: 0.6932 | Time: 128.79s
Train Loss: 0.6883 | Val Loss: 0.6935 | Time: 160.90s
Train Loss: 0.6834 | Val Loss: 0.6875 | Time: 193.04s
Train Loss: 0.6848 | Val Loss: 0.6838 | Time: 225.17s
Train Loss: 0.6927 | Val Loss: 0.6831 | Time: 257.28s
Train Loss: 0.6934 | Val Loss: 0.6835 | Time: 289.37s
Train Loss: 0.6954 | Val Loss: 0.6862 | Time: 321.47s
Train Loss: 0.6932 | Val Loss: 0.6824 | Time: 353.58s
Train Loss: 0.6912 | Val Loss: 0.6826 | Time: 385.62s
Train Loss: 0.6906 | Val Loss: 0.6786 | Time: 417.65s
Train Loss: 0.6876 | Val Loss: 0.6777 | Time: 449.75s
Train Loss: 0.6871 | Val Loss: 0.6795 | Time: 481.86s
Train Loss: 0.6871 | Val Loss: 0.6815 | Time: 514.04s
Train Loss: 0.6858 | Val Loss: 0.6815 | Time: 546.25s
Early stopping...

Epoch 2/3
Train Loss: 0.7366 | Val Loss: 0.6802 | Time: 32.21s
Early stopping...

Epoch 3/3
Train Loss: 0.7029 | Val Loss: 0.6796 | Time: 32.17s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/4
Train Loss: 0.6995 | Val Loss: 0.7028 | Time: 32.40s
Train Loss: 0.7069 | Val Loss: 0.6941 | Time: 64.67s
Train Loss: 0.7019 | Val Loss: 0.6906 | Time: 96.91s
Train Loss: 0.6930 | Val Loss: 0.6887 | Time: 129.15s
Train Loss: 0.6846 | Val Loss: 0.6891 | Time: 161.35s
Train Loss: 0.6868 | Val Loss: 0.6875 | Time: 193.54s
Train Loss: 0.6898 | Val Loss: 0.6860 | Time: 225.82s
Train Loss: 0.6890 | Val Loss: 0.6834 | Time: 258.12s
Train Loss: 0.6886 | Val Loss: 0.6831 | Time: 290.26s
Train Loss: 0.6895 | Val Loss: 0.6809 | Time: 322.32s
Train Loss: 0.6902 | Val Loss: 0.6823 | Time: 354.29s
Train Loss: 0.6896 | Val Loss: 0.6817 | Time: 386.30s
Train Loss: 0.6899 | Val Loss: 0.6804 | Time: 418.44s
Train Loss: 0.6887 | Val Loss: 0.6800 | Time: 450.61s
Train Loss: 0.6886 | Val Loss: 0.6802 | Time: 482.78s
Train Loss: 0.6866 | Val Loss: 0.6808 | Time: 515.07s
Train Loss: 0.6864 | Val Loss: 0.6808 | Time: 547.42s
Early stopping...

Epoch 2/4
Train Loss: 0.6762 | Val Loss: 0.6819 | Time: 32.19s
Early stopping...

Epoch 3/4
Train Loss: 0.7218 | Val Loss: 0.6796 | Time: 32.11s
Train Loss: 0.7346 | Val Loss: 0.6759 | Time: 64.09s
Train Loss: 0.7120 | Val Loss: 0.6726 | Time: 96.10s
Train Loss: 0.6990 | Val Loss: 0.6707 | Time: 128.15s
Train Loss: 0.6861 | Val Loss: 0.6694 | Time: 160.23s
Train Loss: 0.6876 | Val Loss: 0.6682 | Time: 192.29s
Train Loss: 0.6840 | Val Loss: 0.6679 | Time: 224.41s
Train Loss: 0.6818 | Val Loss: 0.6681 | Time: 256.42s
Train Loss: 0.6791 | Val Loss: 0.6667 | Time: 288.49s
Train Loss: 0.6866 | Val Loss: 0.6661 | Time: 320.52s
Train Loss: 0.6844 | Val Loss: 0.6663 | Time: 352.62s
Train Loss: 0.6788 | Val Loss: 0.6679 | Time: 384.73s
Train Loss: 0.6826 | Val Loss: 0.6679 | Time: 416.86s
Early stopping...

Epoch 4/4
Train Loss: 0.7216 | Val Loss: 0.6686 | Time: 32.20s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/2
Train Loss: 0.6886 | Val Loss: 0.6973 | Time: 32.32s
Train Loss: 0.6855 | Val Loss: 0.6942 | Time: 64.59s
Train Loss: 0.6817 | Val Loss: 0.6957 | Time: 96.79s
Train Loss: 0.6927 | Val Loss: 0.6938 | Time: 129.00s
Train Loss: 0.6994 | Val Loss: 0.6948 | Time: 161.18s
Train Loss: 0.6968 | Val Loss: 0.6954 | Time: 193.34s
Train Loss: 0.6966 | Val Loss: 0.6941 | Time: 225.48s
Early stopping...

Epoch 2/2
Train Loss: 0.6913 | Val Loss: 0.6941 | Time: 32.17s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/3
Train Loss: 0.6817 | Val Loss: 0.6996 | Time: 32.26s
Train Loss: 0.6813 | Val Loss: 0.6975 | Time: 64.45s
Train Loss: 0.6915 | Val Loss: 0.6927 | Time: 96.64s
Train Loss: 0.7030 | Val Loss: 0.6853 | Time: 128.79s
Train Loss: 0.7034 | Val Loss: 0.6836 | Time: 160.99s
Train Loss: 0.7043 | Val Loss: 0.6827 | Time: 193.31s
Train Loss: 0.7066 | Val Loss: 0.6820 | Time: 225.38s
Train Loss: 0.7035 | Val Loss: 0.6811 | Time: 257.23s
Train Loss: 0.7024 | Val Loss: 0.6816 | Time: 289.41s
Train Loss: 0.7043 | Val Loss: 0.6799 | Time: 321.40s
Train Loss: 0.7012 | Val Loss: 0.6790 | Time: 353.31s
Train Loss: 0.6989 | Val Loss: 0.6769 | Time: 385.17s
Train Loss: 0.6974 | Val Loss: 0.6757 | Time: 417.02s
Train Loss: 0.6989 | Val Loss: 0.6738 | Time: 448.85s
Train Loss: 0.7001 | Val Loss: 0.6724 | Time: 480.87s
Train Loss: 0.6956 | Val Loss: 0.6722 | Time: 513.00s
Train Loss: 0.6916 | Val Loss: 0.6725 | Time: 545.21s
Train Loss: 0.6901 | Val Loss: 0.6726 | Time: 577.31s
Train Loss: 0.6904 | Val Loss: 0.6734 | Time: 609.40s
Early stopping...

Epoch 2/3
Train Loss: 0.6489 | Val Loss: 0.6760 | Time: 32.07s
Early stopping...

Epoch 3/3
Train Loss: 0.6898 | Val Loss: 0.6781 | Time: 31.98s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/4
Train Loss: 0.6952 | Val Loss: 0.7015 | Time: 32.05s
Train Loss: 0.6862 | Val Loss: 0.7024 | Time: 63.95s
Train Loss: 0.7104 | Val Loss: 0.7006 | Time: 95.92s
Train Loss: 0.6998 | Val Loss: 0.7018 | Time: 128.01s
Train Loss: 0.6987 | Val Loss: 0.7011 | Time: 160.11s
Train Loss: 0.6897 | Val Loss: 0.7003 | Time: 192.22s
Train Loss: 0.6956 | Val Loss: 0.6971 | Time: 224.29s
Train Loss: 0.6960 | Val Loss: 0.6952 | Time: 256.30s
Train Loss: 0.6938 | Val Loss: 0.6936 | Time: 288.11s
Train Loss: 0.6941 | Val Loss: 0.6968 | Time: 319.89s
Train Loss: 0.6983 | Val Loss: 0.6980 | Time: 352.02s
Train Loss: 0.6991 | Val Loss: 0.6961 | Time: 384.18s
Early stopping...

Epoch 2/4
Train Loss: 0.7430 | Val Loss: 0.6959 | Time: 32.08s
Early stopping...

Epoch 3/4
Train Loss: 0.6774 | Val Loss: 0.6947 | Time: 32.10s
Early stopping...

Epoch 4/4
Train Loss: 0.6748 | Val Loss: 0.6920 | Time: 31.98s
Train Loss: 0.6779 | Val Loss: 0.6907 | Time: 63.90s
Train Loss: 0.6842 | Val Loss: 0.6913 | Time: 95.82s
Train Loss: 0.6844 | Val Loss: 0.6914 | Time: 127.74s
Train Loss: 0.6824 | Val Loss: 0.6922 | Time: 159.67s
Early stopping...

Best Params: batch=32, lr=3e-05, epochs=4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/4
Train Loss: 0.5874 | Val Loss: 0.5757

Epoch 2/4
Train Loss: 0.5526 | Val Loss: 0.5556

Epoch 3/4
Train Loss: 0.5447 | Val Loss: 0.5634

Epoch 4/4
Train Loss: 0.5377 | Val Loss: 0.5564

Final Evaluation
Accuracy: 0.6902 | Precision: 0.7729 | Recall: 0.5384
F1 Score: 0.6347
Confusion Matrix:
[[1438  270]
 [ 788  919]]