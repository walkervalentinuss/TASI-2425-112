Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
âœ… GPU detected. Training will use GPU.

Running experiment with batch_size=16, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.6896 | Val Loss: 0.6915 | Time: 6.20s
Train Loss: 0.7019 | Val Loss: 0.6898 | Time: 11.84s
Train Loss: 0.7189 | Val Loss: 0.6921 | Time: 17.13s
Train Loss: 0.7177 | Val Loss: 0.6920 | Time: 22.33s
Train Loss: 0.7134 | Val Loss: 0.6913 | Time: 27.65s
Early stopping...

Epoch 2/2
Train Loss: 0.6894 | Val Loss: 0.6912 | Time: 5.37s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7662 | Val Loss: 0.6960 | Time: 5.28s
Train Loss: 0.7092 | Val Loss: 0.6951 | Time: 10.50s
Train Loss: 0.6988 | Val Loss: 0.7024 | Time: 16.08s
Train Loss: 0.7085 | Val Loss: 0.6986 | Time: 21.37s
Train Loss: 0.7048 | Val Loss: 0.7004 | Time: 26.89s
Early stopping...

Epoch 2/3
Train Loss: 0.7737 | Val Loss: 0.6929 | Time: 5.27s
Train Loss: 0.7499 | Val Loss: 0.6818 | Time: 10.58s
Train Loss: 0.7276 | Val Loss: 0.6770 | Time: 15.75s
Train Loss: 0.7187 | Val Loss: 0.6781 | Time: 20.93s
Train Loss: 0.7110 | Val Loss: 0.6764 | Time: 26.01s
Train Loss: 0.7101 | Val Loss: 0.6788 | Time: 31.18s
Train Loss: 0.7152 | Val Loss: 0.6809 | Time: 36.37s
Train Loss: 0.7138 | Val Loss: 0.6804 | Time: 41.55s
Early stopping...

Epoch 3/3
Train Loss: 0.6724 | Val Loss: 0.6814 | Time: 5.55s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.7074 | Val Loss: 0.6920 | Time: 5.58s
Train Loss: 0.6513 | Val Loss: 0.6853 | Time: 10.86s
Train Loss: 0.6824 | Val Loss: 0.6934 | Time: 16.20s
Train Loss: 0.6738 | Val Loss: 0.6972 | Time: 21.36s
Train Loss: 0.6514 | Val Loss: 0.7116 | Time: 26.59s
Early stopping...

Epoch 2/4
Train Loss: 0.7007 | Val Loss: 0.7030 | Time: 5.22s
Early stopping...

Epoch 3/4
Train Loss: 0.6091 | Val Loss: 0.6934 | Time: 5.27s
Early stopping...

Epoch 4/4
Train Loss: 0.6495 | Val Loss: 0.6856 | Time: 5.09s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.7366 | Val Loss: 0.7272 | Time: 5.21s
Train Loss: 0.7292 | Val Loss: 0.7191 | Time: 10.33s
Train Loss: 0.6983 | Val Loss: 0.7077 | Time: 15.53s
Train Loss: 0.7037 | Val Loss: 0.7031 | Time: 20.67s
Train Loss: 0.7033 | Val Loss: 0.6976 | Time: 25.79s
Train Loss: 0.6939 | Val Loss: 0.6907 | Time: 30.95s
Train Loss: 0.6937 | Val Loss: 0.6998 | Time: 36.08s
Train Loss: 0.6866 | Val Loss: 0.7147 | Time: 41.14s
Train Loss: 0.6712 | Val Loss: 0.7184 | Time: 46.29s
Early stopping...

Epoch 2/2
Train Loss: 0.6249 | Val Loss: 0.7234 | Time: 5.28s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7153 | Val Loss: 0.7093 | Time: 5.24s
Train Loss: 0.7472 | Val Loss: 0.7020 | Time: 10.39s
Train Loss: 0.7508 | Val Loss: 0.6973 | Time: 15.41s
Train Loss: 0.7480 | Val Loss: 0.6940 | Time: 20.52s
Train Loss: 0.7297 | Val Loss: 0.6932 | Time: 25.64s
Train Loss: 0.7219 | Val Loss: 0.6923 | Time: 30.74s
Train Loss: 0.7226 | Val Loss: 0.6925 | Time: 35.90s
Train Loss: 0.7197 | Val Loss: 0.6943 | Time: 41.49s
Train Loss: 0.7201 | Val Loss: 0.6962 | Time: 46.65s
Early stopping...

Epoch 2/3
Train Loss: 0.7116 | Val Loss: 0.6957 | Time: 5.17s
Early stopping...

Epoch 3/3
Train Loss: 0.6307 | Val Loss: 0.6952 | Time: 5.17s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.7139 | Val Loss: 0.6928 | Time: 5.27s
Train Loss: 0.7200 | Val Loss: 0.6972 | Time: 10.41s
Train Loss: 0.7152 | Val Loss: 0.7036 | Time: 15.56s
Train Loss: 0.7373 | Val Loss: 0.7027 | Time: 20.72s
Early stopping...

Epoch 2/4
Train Loss: 0.7153 | Val Loss: 0.7037 | Time: 5.20s
Early stopping...

Epoch 3/4
Train Loss: 0.8047 | Val Loss: 0.7001 | Time: 5.96s
Early stopping...

Epoch 4/4
Train Loss: 0.7243 | Val Loss: 0.6983 | Time: 5.25s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.6847 | Val Loss: 0.7013 | Time: 5.39s
Train Loss: 0.6924 | Val Loss: 0.7009 | Time: 10.72s
Train Loss: 0.6995 | Val Loss: 0.7003 | Time: 16.03s
Train Loss: 0.6998 | Val Loss: 0.7005 | Time: 21.47s
Train Loss: 0.6993 | Val Loss: 0.7036 | Time: 26.76s
Train Loss: 0.7060 | Val Loss: 0.7045 | Time: 31.92s
Early stopping...

Epoch 2/2
Train Loss: 0.7142 | Val Loss: 0.7034 | Time: 5.18s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.6557 | Val Loss: 0.6863 | Time: 5.64s
Train Loss: 0.6854 | Val Loss: 0.6875 | Time: 10.81s
Train Loss: 0.6874 | Val Loss: 0.6877 | Time: 16.18s
Train Loss: 0.6931 | Val Loss: 0.6874 | Time: 21.37s
Early stopping...

Epoch 2/3
Train Loss: 0.6864 | Val Loss: 0.6905 | Time: 5.20s
Early stopping...

Epoch 3/3
Train Loss: 0.6784 | Val Loss: 0.6915 | Time: 5.28s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.6477 | Val Loss: 0.6854 | Time: 5.28s
Train Loss: 0.6951 | Val Loss: 0.6843 | Time: 10.42s
Train Loss: 0.7018 | Val Loss: 0.6815 | Time: 15.60s
Train Loss: 0.7063 | Val Loss: 0.6811 | Time: 20.75s
Train Loss: 0.7124 | Val Loss: 0.6803 | Time: 25.94s
Train Loss: 0.7165 | Val Loss: 0.6797 | Time: 31.16s
Train Loss: 0.7124 | Val Loss: 0.6795 | Time: 36.35s
Train Loss: 0.7034 | Val Loss: 0.6776 | Time: 41.50s
Train Loss: 0.6987 | Val Loss: 0.6755 | Time: 46.63s
Train Loss: 0.6976 | Val Loss: 0.6736 | Time: 51.78s
Train Loss: 0.6949 | Val Loss: 0.6734 | Time: 57.15s
Train Loss: 0.6913 | Val Loss: 0.6745 | Time: 62.25s
Train Loss: 0.6920 | Val Loss: 0.6755 | Time: 67.49s
Train Loss: 0.6908 | Val Loss: 0.6769 | Time: 72.66s
Early stopping...

Epoch 2/4
Train Loss: 0.6622 | Val Loss: 0.6783 | Time: 5.15s
Early stopping...

Epoch 3/4
Train Loss: 0.7055 | Val Loss: 0.6802 | Time: 5.16s
Early stopping...

Epoch 4/4
Train Loss: 0.6910 | Val Loss: 0.6822 | Time: 5.17s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.7736 | Val Loss: 0.6929 | Time: 3.67s
Train Loss: 0.7163 | Val Loss: 0.6747 | Time: 7.22s
Train Loss: 0.7126 | Val Loss: 0.6831 | Time: 10.76s
Train Loss: 0.7088 | Val Loss: 0.6861 | Time: 14.29s
Train Loss: 0.7041 | Val Loss: 0.6901 | Time: 17.82s
Early stopping...

Epoch 2/2
Train Loss: 0.6954 | Val Loss: 0.6864 | Time: 3.61s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7554 | Val Loss: 0.6933 | Time: 3.64s
Train Loss: 0.7365 | Val Loss: 0.6905 | Time: 7.23s
Train Loss: 0.7280 | Val Loss: 0.6899 | Time: 10.82s
Train Loss: 0.7237 | Val Loss: 0.6947 | Time: 14.42s
Train Loss: 0.7186 | Val Loss: 0.6941 | Time: 18.16s
Train Loss: 0.7149 | Val Loss: 0.6902 | Time: 21.71s
Early stopping...

Epoch 2/3
Train Loss: 0.6948 | Val Loss: 0.6870 | Time: 3.81s
Train Loss: 0.6962 | Val Loss: 0.6820 | Time: 7.35s
Train Loss: 0.6944 | Val Loss: 0.6806 | Time: 10.79s
Train Loss: 0.6976 | Val Loss: 0.6812 | Time: 14.24s
Train Loss: 0.6924 | Val Loss: 0.6806 | Time: 17.69s
Train Loss: 0.6873 | Val Loss: 0.6748 | Time: 21.11s
Train Loss: 0.6810 | Val Loss: 0.6683 | Time: 24.54s
Train Loss: 0.6783 | Val Loss: 0.6638 | Time: 27.98s
Train Loss: 0.6752 | Val Loss: 0.6611 | Time: 31.64s
Train Loss: 0.6708 | Val Loss: 0.6575 | Time: 35.25s
Train Loss: 0.6625 | Val Loss: 0.6548 | Time: 38.65s
Train Loss: 0.6616 | Val Loss: 0.6533 | Time: 42.04s
Train Loss: 0.6634 | Val Loss: 0.6496 | Time: 45.48s
Train Loss: 0.6639 | Val Loss: 0.6452 | Time: 49.02s
Train Loss: 0.6698 | Val Loss: 0.6434 | Time: 52.48s
Train Loss: 0.6678 | Val Loss: 0.6449 | Time: 55.90s
Train Loss: 0.6702 | Val Loss: 0.6556 | Time: 59.36s
Train Loss: 0.6717 | Val Loss: 0.6644 | Time: 62.82s
Early stopping...

Epoch 3/3
Train Loss: 0.5930 | Val Loss: 0.6654 | Time: 3.49s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.6604 | Val Loss: 0.6927 | Time: 3.66s
Train Loss: 0.6762 | Val Loss: 0.6954 | Time: 7.26s
Train Loss: 0.6759 | Val Loss: 0.6947 | Time: 10.93s
Train Loss: 0.6943 | Val Loss: 0.6808 | Time: 14.55s
Train Loss: 0.6983 | Val Loss: 0.6780 | Time: 18.11s
Train Loss: 0.6987 | Val Loss: 0.6811 | Time: 21.73s
Train Loss: 0.7006 | Val Loss: 0.6844 | Time: 25.26s
Train Loss: 0.6950 | Val Loss: 0.6869 | Time: 28.88s
Early stopping...

Epoch 2/4
Train Loss: 0.7086 | Val Loss: 0.6844 | Time: 3.67s
Early stopping...

Epoch 3/4
Train Loss: 0.6849 | Val Loss: 0.6796 | Time: 3.46s
Early stopping...

Epoch 4/4
Train Loss: 0.6754 | Val Loss: 0.6802 | Time: 3.59s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.7380 | Val Loss: 0.6975 | Time: 3.77s
Train Loss: 0.7214 | Val Loss: 0.6939 | Time: 7.38s
Train Loss: 0.7082 | Val Loss: 0.6948 | Time: 11.02s
Train Loss: 0.7049 | Val Loss: 0.6996 | Time: 14.46s
Train Loss: 0.6982 | Val Loss: 0.7031 | Time: 17.91s
Early stopping...

Epoch 2/2
Train Loss: 0.6939 | Val Loss: 0.7027 | Time: 3.52s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7268 | Val Loss: 0.6953 | Time: 3.74s
Train Loss: 0.7190 | Val Loss: 0.6936 | Time: 7.23s
Train Loss: 0.7033 | Val Loss: 0.6958 | Time: 10.64s
Train Loss: 0.6983 | Val Loss: 0.6940 | Time: 14.06s
Train Loss: 0.7072 | Val Loss: 0.6928 | Time: 17.52s
Train Loss: 0.7028 | Val Loss: 0.6907 | Time: 20.94s
Train Loss: 0.7027 | Val Loss: 0.6918 | Time: 24.37s
Train Loss: 0.6987 | Val Loss: 0.6938 | Time: 27.82s
Train Loss: 0.6998 | Val Loss: 0.6934 | Time: 31.26s
Early stopping...

Epoch 2/3
Train Loss: 0.7359 | Val Loss: 0.6914 | Time: 3.64s
Early stopping...

Epoch 3/3
Train Loss: 0.6635 | Val Loss: 0.6917 | Time: 3.66s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.7194 | Val Loss: 0.7018 | Time: 3.79s
Train Loss: 0.7121 | Val Loss: 0.7003 | Time: 7.39s
Train Loss: 0.7026 | Val Loss: 0.6960 | Time: 11.11s
Train Loss: 0.6960 | Val Loss: 0.6923 | Time: 14.82s
Train Loss: 0.6930 | Val Loss: 0.6879 | Time: 18.24s
Train Loss: 0.6907 | Val Loss: 0.6844 | Time: 21.73s
Train Loss: 0.6887 | Val Loss: 0.6801 | Time: 25.35s
Train Loss: 0.6883 | Val Loss: 0.6774 | Time: 29.02s
Train Loss: 0.6891 | Val Loss: 0.6748 | Time: 33.31s
Train Loss: 0.6891 | Val Loss: 0.6743 | Time: 36.81s
Train Loss: 0.6862 | Val Loss: 0.6737 | Time: 40.41s
Train Loss: 0.6861 | Val Loss: 0.6737 | Time: 44.05s
Train Loss: 0.6861 | Val Loss: 0.6723 | Time: 47.49s
Train Loss: 0.6863 | Val Loss: 0.6715 | Time: 51.18s
Train Loss: 0.6852 | Val Loss: 0.6724 | Time: 54.89s
Train Loss: 0.6843 | Val Loss: 0.6739 | Time: 58.53s
Train Loss: 0.6824 | Val Loss: 0.6740 | Time: 62.08s
Early stopping...

Epoch 2/4
Train Loss: 0.6709 | Val Loss: 0.6723 | Time: 3.63s
Early stopping...

Epoch 3/4
Train Loss: 0.6698 | Val Loss: 0.6704 | Time: 3.62s
Train Loss: 0.6524 | Val Loss: 0.6675 | Time: 7.26s
Train Loss: 0.6751 | Val Loss: 0.6654 | Time: 10.92s
Train Loss: 0.6818 | Val Loss: 0.6636 | Time: 14.54s
Train Loss: 0.6744 | Val Loss: 0.6630 | Time: 18.17s
Train Loss: 0.6706 | Val Loss: 0.6633 | Time: 21.71s
Train Loss: 0.6657 | Val Loss: 0.6638 | Time: 25.39s
Train Loss: 0.6632 | Val Loss: 0.6650 | Time: 28.97s
Early stopping...

Epoch 4/4
Train Loss: 0.6778 | Val Loss: 0.6661 | Time: 3.46s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.7034 | Val Loss: 0.6988 | Time: 3.56s
Train Loss: 0.6977 | Val Loss: 0.6946 | Time: 7.10s
Train Loss: 0.6844 | Val Loss: 0.6939 | Time: 10.74s
Train Loss: 0.6844 | Val Loss: 0.6915 | Time: 14.43s
Train Loss: 0.6905 | Val Loss: 0.6894 | Time: 17.97s
Train Loss: 0.6945 | Val Loss: 0.6885 | Time: 21.46s
Train Loss: 0.6902 | Val Loss: 0.6891 | Time: 25.00s
Train Loss: 0.6917 | Val Loss: 0.6890 | Time: 28.67s
Train Loss: 0.6906 | Val Loss: 0.6883 | Time: 32.07s
Train Loss: 0.6907 | Val Loss: 0.6871 | Time: 35.52s
Train Loss: 0.6912 | Val Loss: 0.6867 | Time: 39.10s
Train Loss: 0.6907 | Val Loss: 0.6864 | Time: 42.67s
Train Loss: 0.6879 | Val Loss: 0.6861 | Time: 46.31s
Train Loss: 0.6870 | Val Loss: 0.6846 | Time: 50.05s
Train Loss: 0.6883 | Val Loss: 0.6836 | Time: 53.68s
Train Loss: 0.6863 | Val Loss: 0.6825 | Time: 57.32s
Train Loss: 0.6846 | Val Loss: 0.6833 | Time: 60.93s
Train Loss: 0.6839 | Val Loss: 0.6833 | Time: 64.59s
Train Loss: 0.6823 | Val Loss: 0.6833 | Time: 68.28s
Early stopping...

Epoch 2/2
Train Loss: 0.6408 | Val Loss: 0.6852 | Time: 3.66s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.8016 | Val Loss: 0.7156 | Time: 3.73s
Train Loss: 0.7644 | Val Loss: 0.7056 | Time: 7.17s
Train Loss: 0.7632 | Val Loss: 0.6963 | Time: 10.60s
Train Loss: 0.7571 | Val Loss: 0.6921 | Time: 14.25s
Train Loss: 0.7383 | Val Loss: 0.6930 | Time: 17.94s
Train Loss: 0.7331 | Val Loss: 0.6923 | Time: 21.47s
Train Loss: 0.7310 | Val Loss: 0.6948 | Time: 24.90s
Early stopping...

Epoch 2/3
Train Loss: 0.6804 | Val Loss: 0.6958 | Time: 3.50s
Early stopping...

Epoch 3/3
Train Loss: 0.7013 | Val Loss: 0.6959 | Time: 3.71s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.7113 | Val Loss: 0.7096 | Time: 3.73s
Train Loss: 0.7088 | Val Loss: 0.7056 | Time: 7.35s
Train Loss: 0.7002 | Val Loss: 0.7028 | Time: 10.80s
Train Loss: 0.7027 | Val Loss: 0.6990 | Time: 14.38s
Train Loss: 0.7034 | Val Loss: 0.6977 | Time: 17.99s
Train Loss: 0.7056 | Val Loss: 0.6948 | Time: 21.58s
Train Loss: 0.6985 | Val Loss: 0.6929 | Time: 25.16s
Train Loss: 0.6982 | Val Loss: 0.6920 | Time: 28.87s
Train Loss: 0.6943 | Val Loss: 0.6902 | Time: 32.45s
Train Loss: 0.6930 | Val Loss: 0.6879 | Time: 35.87s
Train Loss: 0.6906 | Val Loss: 0.6878 | Time: 39.33s
Train Loss: 0.6891 | Val Loss: 0.6896 | Time: 42.87s
Train Loss: 0.6884 | Val Loss: 0.6916 | Time: 46.35s
Train Loss: 0.6884 | Val Loss: 0.6921 | Time: 49.96s
Early stopping...

Epoch 2/4
Train Loss: 0.6369 | Val Loss: 0.6931 | Time: 3.72s
Early stopping...

Epoch 3/4
Train Loss: 0.6530 | Val Loss: 0.6950 | Time: 3.75s
Early stopping...

Epoch 4/4
Train Loss: 0.7336 | Val Loss: 0.6933 | Time: 3.65s
Early stopping...

Best Params: batch=32, lr=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.6559 | Val Loss: 0.6697

Epoch 2/3
Train Loss: 0.6543 | Val Loss: 0.6391

Epoch 3/3
Train Loss: 0.5673 | Val Loss: 0.6743

Final Evaluation
Accuracy: 0.7306 | Precision: 0.8378 | Recall: 0.5688
F1 Score: 0.6776
Confusion Matrix:
[[98 12]
 [47 62]]
