Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-05-15 19:32:34.567015: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
WARNING:absl:Found untraced functions such as serving, embeddings_layer_call_fn, embeddings_layer_call_and_return_conditional_losses, encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses while saving (showing 5 of 41). These functions will not be directly callable after loading.
GPU detected. Training will use GPU.

Running experiment with batch_size=16, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.6834 | Val Loss: 0.7134 | Time: 64.61s
Train Loss: 0.7298 | Val Loss: 0.7094 | Time: 128.08s
Train Loss: 0.7072 | Val Loss: 0.7157 | Time: 191.26s
Train Loss: 0.7389 | Val Loss: 0.6973 | Time: 253.82s
Train Loss: 0.7177 | Val Loss: 0.6958 | Time: 316.79s
Train Loss: 0.7129 | Val Loss: 0.6901 | Time: 379.72s
Train Loss: 0.7161 | Val Loss: 0.6912 | Time: 442.42s
Train Loss: 0.7141 | Val Loss: 0.6927 | Time: 505.13s
Train Loss: 0.7178 | Val Loss: 0.6923 | Time: 567.81s
Early stopping...

Epoch 2/2
Train Loss: 0.7093 | Val Loss: 0.6929 | Time: 62.88s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7622 | Val Loss: 0.7025 | Time: 63.21s
Train Loss: 0.7291 | Val Loss: 0.6921 | Time: 126.50s
Train Loss: 0.7210 | Val Loss: 0.6889 | Time: 189.63s
Train Loss: 0.7064 | Val Loss: 0.6883 | Time: 252.55s
Train Loss: 0.6967 | Val Loss: 0.6915 | Time: 315.56s
Train Loss: 0.6965 | Val Loss: 0.6967 | Time: 378.41s
Train Loss: 0.7095 | Val Loss: 0.6965 | Time: 441.24s
Early stopping...

Epoch 2/3
Train Loss: 0.7066 | Val Loss: 0.6961 | Time: 63.46s
Early stopping...

Epoch 3/3
Train Loss: 0.6912 | Val Loss: 0.6938 | Time: 63.30s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.6957 | Val Loss: 0.7021 | Time: 63.28s
Train Loss: 0.7023 | Val Loss: 0.6994 | Time: 126.60s
Train Loss: 0.7186 | Val Loss: 0.6899 | Time: 189.95s
Train Loss: 0.7097 | Val Loss: 0.6820 | Time: 253.00s
Train Loss: 0.6969 | Val Loss: 0.6801 | Time: 316.00s
Train Loss: 0.6868 | Val Loss: 0.6822 | Time: 378.97s
Train Loss: 0.6922 | Val Loss: 0.6887 | Time: 441.95s
Train Loss: 0.6889 | Val Loss: 0.6973 | Time: 504.99s
Early stopping...

Epoch 2/4
Train Loss: 0.6861 | Val Loss: 0.6942 | Time: 63.18s
Early stopping...

Epoch 3/4
Train Loss: 0.6861 | Val Loss: 0.6905 | Time: 63.12s
Early stopping...

Epoch 4/4
Train Loss: 0.6307 | Val Loss: 0.6963 | Time: 63.34s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.6915 | Val Loss: 0.7065 | Time: 63.55s
Train Loss: 0.7089 | Val Loss: 0.6984 | Time: 126.98s
Train Loss: 0.7053 | Val Loss: 0.6917 | Time: 190.04s
Train Loss: 0.7198 | Val Loss: 0.6863 | Time: 252.89s
Train Loss: 0.7058 | Val Loss: 0.6815 | Time: 315.89s
Train Loss: 0.6973 | Val Loss: 0.6785 | Time: 378.69s
Train Loss: 0.6934 | Val Loss: 0.6762 | Time: 441.62s
Train Loss: 0.7005 | Val Loss: 0.6735 | Time: 504.45s
Train Loss: 0.6974 | Val Loss: 0.6718 | Time: 567.47s
Train Loss: 0.6985 | Val Loss: 0.6712 | Time: 630.57s
Train Loss: 0.7027 | Val Loss: 0.6686 | Time: 693.86s
Train Loss: 0.7069 | Val Loss: 0.6703 | Time: 756.96s
Train Loss: 0.6978 | Val Loss: 0.6737 | Time: 821.17s
Train Loss: 0.6979 | Val Loss: 0.6737 | Time: 886.94s
Early stopping...

Epoch 2/2
Train Loss: 0.5853 | Val Loss: 0.6746 | Time: 63.22s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7864 | Val Loss: 0.7146 | Time: 65.26s
Train Loss: 0.7380 | Val Loss: 0.7036 | Time: 129.31s
Train Loss: 0.7426 | Val Loss: 0.6941 | Time: 194.53s
Train Loss: 0.7262 | Val Loss: 0.6944 | Time: 263.79s
Train Loss: 0.7216 | Val Loss: 0.6903 | Time: 328.61s
Train Loss: 0.7099 | Val Loss: 0.6881 | Time: 397.16s
Train Loss: 0.7000 | Val Loss: 0.6878 | Time: 463.04s
Train Loss: 0.7011 | Val Loss: 0.6902 | Time: 528.30s
Train Loss: 0.7060 | Val Loss: 0.6885 | Time: 594.83s
Train Loss: 0.7048 | Val Loss: 0.6864 | Time: 658.94s
Train Loss: 0.7048 | Val Loss: 0.6819 | Time: 724.13s
Train Loss: 0.6989 | Val Loss: 0.6790 | Time: 788.01s
Train Loss: 0.6951 | Val Loss: 0.6771 | Time: 854.03s
Train Loss: 0.6929 | Val Loss: 0.6774 | Time: 919.77s
Train Loss: 0.6905 | Val Loss: 0.6783 | Time: 985.54s
Train Loss: 0.6897 | Val Loss: 0.6803 | Time: 1050.80s
Early stopping...

Epoch 2/3
Train Loss: 0.6385 | Val Loss: 0.6837 | Time: 65.43s
Early stopping...

Epoch 3/3
Train Loss: 0.6950 | Val Loss: 0.6879 | Time: 64.59s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.7067 | Val Loss: 0.6964 | Time: 66.09s
Train Loss: 0.7028 | Val Loss: 0.6923 | Time: 130.77s
Train Loss: 0.6944 | Val Loss: 0.6908 | Time: 194.20s
Train Loss: 0.6927 | Val Loss: 0.6910 | Time: 257.93s
Train Loss: 0.6943 | Val Loss: 0.6919 | Time: 321.39s
Train Loss: 0.6938 | Val Loss: 0.6939 | Time: 386.27s
Early stopping...

Epoch 2/4
Train Loss: 0.6521 | Val Loss: 0.6940 | Time: 68.02s
Early stopping...

Epoch 3/4
Train Loss: 0.7041 | Val Loss: 0.6912 | Time: 63.27s
Early stopping...

Epoch 4/4
Train Loss: 0.6750 | Val Loss: 0.6919 | Time: 63.27s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.6680 | Val Loss: 0.6974 | Time: 63.44s
Train Loss: 0.6923 | Val Loss: 0.6910 | Time: 127.04s
Train Loss: 0.6954 | Val Loss: 0.6928 | Time: 190.25s
Train Loss: 0.7092 | Val Loss: 0.6925 | Time: 253.13s
Train Loss: 0.7011 | Val Loss: 0.6935 | Time: 315.74s
Early stopping...

Epoch 2/2
Train Loss: 0.6968 | Val Loss: 0.6926 | Time: 62.53s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.6726 | Val Loss: 0.7013 | Time: 62.92s
Train Loss: 0.7162 | Val Loss: 0.7045 | Time: 126.02s
Train Loss: 0.7173 | Val Loss: 0.7068 | Time: 188.82s
Train Loss: 0.7156 | Val Loss: 0.7011 | Time: 251.25s
Train Loss: 0.7051 | Val Loss: 0.6986 | Time: 313.56s
Train Loss: 0.6977 | Val Loss: 0.6983 | Time: 375.63s
Train Loss: 0.6953 | Val Loss: 0.6967 | Time: 437.75s
Train Loss: 0.6997 | Val Loss: 0.6965 | Time: 500.29s
Train Loss: 0.6970 | Val Loss: 0.6963 | Time: 562.81s
Train Loss: 0.6935 | Val Loss: 0.6991 | Time: 624.96s
Train Loss: 0.6874 | Val Loss: 0.7009 | Time: 687.20s
Train Loss: 0.6894 | Val Loss: 0.7006 | Time: 749.37s
Early stopping...

Epoch 2/3
Train Loss: 0.6867 | Val Loss: 0.6993 | Time: 62.66s
Early stopping...

Epoch 3/3
Train Loss: 0.7875 | Val Loss: 0.6952 | Time: 62.60s
Train Loss: 0.7104 | Val Loss: 0.6906 | Time: 125.16s
Train Loss: 0.7062 | Val Loss: 0.6909 | Time: 187.61s
Train Loss: 0.7037 | Val Loss: 0.6914 | Time: 250.01s
Train Loss: 0.6961 | Val Loss: 0.6914 | Time: 312.48s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.6296 | Val Loss: 0.7160 | Time: 62.94s
Train Loss: 0.6455 | Val Loss: 0.7227 | Time: 125.23s
Train Loss: 0.7175 | Val Loss: 0.7195 | Time: 187.81s
Train Loss: 0.7419 | Val Loss: 0.7203 | Time: 250.04s
Early stopping...

Epoch 2/4
Train Loss: 0.7887 | Val Loss: 0.7165 | Time: 62.46s
Early stopping...

Epoch 3/4
Train Loss: 0.7849 | Val Loss: 0.7088 | Time: 62.28s
Train Loss: 0.7369 | Val Loss: 0.7017 | Time: 124.62s
Train Loss: 0.7295 | Val Loss: 0.6952 | Time: 186.96s
Train Loss: 0.7162 | Val Loss: 0.6924 | Time: 249.44s
Train Loss: 0.7116 | Val Loss: 0.6905 | Time: 311.80s
Train Loss: 0.7066 | Val Loss: 0.6891 | Time: 373.98s
Train Loss: 0.7101 | Val Loss: 0.6882 | Time: 435.97s
Train Loss: 0.7091 | Val Loss: 0.6882 | Time: 497.74s
Train Loss: 0.7060 | Val Loss: 0.6889 | Time: 559.57s
Train Loss: 0.7083 | Val Loss: 0.6900 | Time: 621.28s
Early stopping...

Epoch 4/4
Train Loss: 0.6587 | Val Loss: 0.6911 | Time: 62.26s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.6820 | Val Loss: 0.6890 | Time: 31.77s
Train Loss: 0.6838 | Val Loss: 0.6856 | Time: 63.32s
Train Loss: 0.6650 | Val Loss: 0.7027 | Time: 94.68s
Train Loss: 0.6784 | Val Loss: 0.6870 | Time: 126.01s
Train Loss: 0.6725 | Val Loss: 0.6790 | Time: 157.37s
Train Loss: 0.6689 | Val Loss: 0.6807 | Time: 188.78s
Train Loss: 0.6706 | Val Loss: 0.6831 | Time: 220.26s
Train Loss: 0.6689 | Val Loss: 0.6814 | Time: 251.59s
Early stopping...

Epoch 2/2
Train Loss: 0.6733 | Val Loss: 0.6773 | Time: 31.54s
Train Loss: 0.7619 | Val Loss: 0.6729 | Time: 63.02s
Train Loss: 0.7094 | Val Loss: 0.6709 | Time: 94.40s
Train Loss: 0.6829 | Val Loss: 0.6692 | Time: 125.81s
Train Loss: 0.6832 | Val Loss: 0.6787 | Time: 157.21s
Train Loss: 0.6775 | Val Loss: 0.6669 | Time: 188.73s
Train Loss: 0.6775 | Val Loss: 0.6634 | Time: 220.00s
Train Loss: 0.6797 | Val Loss: 0.6606 | Time: 251.29s
Train Loss: 0.6780 | Val Loss: 0.6575 | Time: 282.69s
Train Loss: 0.6749 | Val Loss: 0.6554 | Time: 314.02s
Train Loss: 0.6676 | Val Loss: 0.6554 | Time: 345.35s
Train Loss: 0.6660 | Val Loss: 0.6551 | Time: 376.64s
Train Loss: 0.6609 | Val Loss: 0.6544 | Time: 408.03s
Train Loss: 0.6594 | Val Loss: 0.6539 | Time: 439.54s
Train Loss: 0.6575 | Val Loss: 0.6526 | Time: 470.96s
Train Loss: 0.6584 | Val Loss: 0.6499 | Time: 502.32s
Train Loss: 0.6603 | Val Loss: 0.6485 | Time: 533.66s
Train Loss: 0.6578 | Val Loss: 0.6482 | Time: 564.93s
Train Loss: 0.6576 | Val Loss: 0.6465 | Time: 596.26s
Train Loss: 0.6565 | Val Loss: 0.6451 | Time: 627.54s
Train Loss: 0.6567 | Val Loss: 0.6458 | Time: 658.89s
Train Loss: 0.6574 | Val Loss: 0.6449 | Time: 690.22s
Train Loss: 0.6584 | Val Loss: 0.6420 | Time: 721.55s
Train Loss: 0.6570 | Val Loss: 0.6434 | Time: 752.84s
Train Loss: 0.6571 | Val Loss: 0.6474 | Time: 784.19s
Train Loss: 0.6576 | Val Loss: 0.6513 | Time: 815.51s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7784 | Val Loss: 0.6946 | Time: 31.65s
Train Loss: 0.7234 | Val Loss: 0.6949 | Time: 63.06s
Train Loss: 0.7233 | Val Loss: 0.6962 | Time: 94.36s
Train Loss: 0.7174 | Val Loss: 0.6927 | Time: 125.63s
Train Loss: 0.7075 | Val Loss: 0.6890 | Time: 156.93s
Train Loss: 0.7100 | Val Loss: 0.6879 | Time: 188.37s
Train Loss: 0.7076 | Val Loss: 0.6889 | Time: 219.84s
Train Loss: 0.7007 | Val Loss: 0.6880 | Time: 251.32s
Train Loss: 0.6928 | Val Loss: 0.6856 | Time: 282.77s
Train Loss: 0.6868 | Val Loss: 0.6850 | Time: 314.09s
Train Loss: 0.6866 | Val Loss: 0.6769 | Time: 345.42s
Train Loss: 0.6859 | Val Loss: 0.6728 | Time: 376.86s
Train Loss: 0.6811 | Val Loss: 0.6751 | Time: 408.17s
Train Loss: 0.6768 | Val Loss: 0.6713 | Time: 439.53s
Train Loss: 0.6762 | Val Loss: 0.6704 | Time: 470.88s
Train Loss: 0.6735 | Val Loss: 0.6703 | Time: 502.21s
Train Loss: 0.6737 | Val Loss: 0.6665 | Time: 533.54s
Train Loss: 0.6732 | Val Loss: 0.6659 | Time: 564.89s
Train Loss: 0.6741 | Val Loss: 0.6643 | Time: 596.18s
Train Loss: 0.6756 | Val Loss: 0.6595 | Time: 627.53s
Train Loss: 0.6761 | Val Loss: 0.6573 | Time: 658.84s
Train Loss: 0.6757 | Val Loss: 0.6560 | Time: 690.18s
Train Loss: 0.6760 | Val Loss: 0.6552 | Time: 721.49s
Train Loss: 0.6760 | Val Loss: 0.6566 | Time: 752.81s
Train Loss: 0.6746 | Val Loss: 0.6613 | Time: 784.16s
Train Loss: 0.6732 | Val Loss: 0.6646 | Time: 815.50s
Early stopping...

Epoch 2/3
Train Loss: 0.7227 | Val Loss: 0.6600 | Time: 31.57s
Early stopping...

Epoch 3/3
Train Loss: 0.6780 | Val Loss: 0.6545 | Time: 31.55s
Train Loss: 0.6447 | Val Loss: 0.6520 | Time: 62.93s
Train Loss: 0.6423 | Val Loss: 0.6501 | Time: 94.33s
Train Loss: 0.6342 | Val Loss: 0.6509 | Time: 125.70s
Train Loss: 0.6326 | Val Loss: 0.6526 | Time: 157.02s
Train Loss: 0.6365 | Val Loss: 0.6534 | Time: 188.36s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.7461 | Val Loss: 0.6952 | Time: 31.82s
Train Loss: 0.7335 | Val Loss: 0.6932 | Time: 63.50s
Train Loss: 0.7139 | Val Loss: 0.6907 | Time: 95.01s
Train Loss: 0.7048 | Val Loss: 0.6906 | Time: 126.36s
Train Loss: 0.7052 | Val Loss: 0.6878 | Time: 157.66s
Train Loss: 0.6982 | Val Loss: 0.6879 | Time: 189.09s
Train Loss: 0.6989 | Val Loss: 0.6925 | Time: 220.82s
Train Loss: 0.7029 | Val Loss: 0.6905 | Time: 252.17s
Early stopping...

Epoch 2/4
Train Loss: 0.6794 | Val Loss: 0.6823 | Time: 31.61s
Train Loss: 0.6582 | Val Loss: 0.6804 | Time: 63.01s
Train Loss: 0.6774 | Val Loss: 0.6825 | Time: 94.52s
Train Loss: 0.6754 | Val Loss: 0.6841 | Time: 126.10s
Train Loss: 0.6744 | Val Loss: 0.6868 | Time: 157.58s
Early stopping...

Epoch 3/4
Train Loss: 0.6346 | Val Loss: 0.6908 | Time: 31.63s
Early stopping...

Epoch 4/4
Train Loss: 0.6846 | Val Loss: 0.6912 | Time: 31.68s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.6534 | Val Loss: 0.6934 | Time: 31.70s
Train Loss: 0.6635 | Val Loss: 0.6987 | Time: 63.14s
Train Loss: 0.6649 | Val Loss: 0.7031 | Time: 94.52s
Train Loss: 0.6730 | Val Loss: 0.7001 | Time: 125.92s
Early stopping...

Epoch 2/2
Train Loss: 0.6690 | Val Loss: 0.6962 | Time: 31.57s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7071 | Val Loss: 0.6865 | Time: 31.63s
Train Loss: 0.6861 | Val Loss: 0.6846 | Time: 63.11s
Train Loss: 0.6823 | Val Loss: 0.6857 | Time: 94.49s
Train Loss: 0.6744 | Val Loss: 0.6844 | Time: 125.78s
Train Loss: 0.6817 | Val Loss: 0.6827 | Time: 157.24s
Train Loss: 0.6840 | Val Loss: 0.6829 | Time: 188.69s
Train Loss: 0.6751 | Val Loss: 0.6856 | Time: 220.08s
Train Loss: 0.6752 | Val Loss: 0.6855 | Time: 251.32s
Early stopping...

Epoch 2/3
Train Loss: 0.6148 | Val Loss: 0.6808 | Time: 31.52s
Train Loss: 0.6293 | Val Loss: 0.6791 | Time: 62.87s
Train Loss: 0.6533 | Val Loss: 0.6804 | Time: 94.28s
Train Loss: 0.6609 | Val Loss: 0.6795 | Time: 125.80s
Train Loss: 0.6568 | Val Loss: 0.6761 | Time: 157.33s
Train Loss: 0.6488 | Val Loss: 0.6752 | Time: 188.85s
Train Loss: 0.6450 | Val Loss: 0.6783 | Time: 220.30s
Train Loss: 0.6510 | Val Loss: 0.6761 | Time: 251.76s
Train Loss: 0.6595 | Val Loss: 0.6761 | Time: 283.15s
Early stopping...

Epoch 3/3
Train Loss: 0.6606 | Val Loss: 0.6754 | Time: 31.53s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.7300 | Val Loss: 0.6991 | Time: 31.74s
Train Loss: 0.7133 | Val Loss: 0.7022 | Time: 63.18s
Train Loss: 0.7008 | Val Loss: 0.7068 | Time: 94.51s
Train Loss: 0.6992 | Val Loss: 0.7040 | Time: 125.97s
Early stopping...

Epoch 2/4
Train Loss: 0.6753 | Val Loss: 0.7086 | Time: 31.79s
Early stopping...

Epoch 3/4
Train Loss: 0.6634 | Val Loss: 0.7136 | Time: 31.52s
Early stopping...

Epoch 4/4
Train Loss: 0.7160 | Val Loss: 0.7152 | Time: 31.59s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.8005 | Val Loss: 0.7082 | Time: 31.71s
Train Loss: 0.7660 | Val Loss: 0.7000 | Time: 63.22s
Train Loss: 0.7563 | Val Loss: 0.6990 | Time: 94.64s
Train Loss: 0.7392 | Val Loss: 0.6981 | Time: 125.99s
Train Loss: 0.7278 | Val Loss: 0.6964 | Time: 157.37s
Train Loss: 0.7212 | Val Loss: 0.6969 | Time: 188.85s
Train Loss: 0.7201 | Val Loss: 0.6940 | Time: 220.37s
Train Loss: 0.7133 | Val Loss: 0.6958 | Time: 251.76s
Train Loss: 0.7146 | Val Loss: 0.6965 | Time: 283.13s
Train Loss: 0.7107 | Val Loss: 0.6948 | Time: 314.53s
Early stopping...

Epoch 2/2
Train Loss: 0.7068 | Val Loss: 0.6900 | Time: 31.58s
Train Loss: 0.6903 | Val Loss: 0.6878 | Time: 63.00s
Train Loss: 0.6882 | Val Loss: 0.6855 | Time: 94.42s
Train Loss: 0.6920 | Val Loss: 0.6847 | Time: 125.82s
Train Loss: 0.6947 | Val Loss: 0.6840 | Time: 157.18s
Train Loss: 0.6927 | Val Loss: 0.6831 | Time: 188.63s
Train Loss: 0.6914 | Val Loss: 0.6820 | Time: 220.02s
Train Loss: 0.6887 | Val Loss: 0.6809 | Time: 251.37s
Train Loss: 0.6868 | Val Loss: 0.6809 | Time: 282.80s
Train Loss: 0.6865 | Val Loss: 0.6795 | Time: 314.18s
Train Loss: 0.6890 | Val Loss: 0.6787 | Time: 345.56s
Train Loss: 0.6836 | Val Loss: 0.6768 | Time: 376.90s
Train Loss: 0.6817 | Val Loss: 0.6752 | Time: 408.25s
Train Loss: 0.6839 | Val Loss: 0.6745 | Time: 439.59s
Train Loss: 0.6818 | Val Loss: 0.6736 | Time: 471.01s
Train Loss: 0.6787 | Val Loss: 0.6752 | Time: 502.40s
Train Loss: 0.6783 | Val Loss: 0.6749 | Time: 533.76s
Train Loss: 0.6770 | Val Loss: 0.6734 | Time: 565.11s
Train Loss: 0.6768 | Val Loss: 0.6725 | Time: 596.44s
Train Loss: 0.6784 | Val Loss: 0.6712 | Time: 627.77s
Train Loss: 0.6756 | Val Loss: 0.6700 | Time: 659.12s
Train Loss: 0.6774 | Val Loss: 0.6695 | Time: 690.55s
Train Loss: 0.6784 | Val Loss: 0.6700 | Time: 721.85s
Train Loss: 0.6764 | Val Loss: 0.6699 | Time: 753.27s
Train Loss: 0.6750 | Val Loss: 0.6699 | Time: 784.82s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.6886 | Val Loss: 0.6935 | Time: 31.74s
Train Loss: 0.6918 | Val Loss: 0.6947 | Time: 63.39s
Train Loss: 0.6925 | Val Loss: 0.6954 | Time: 94.91s
Train Loss: 0.7014 | Val Loss: 0.6962 | Time: 126.30s
Early stopping...

Epoch 2/3
Train Loss: 0.7561 | Val Loss: 0.6958 | Time: 31.74s
Early stopping...

Epoch 3/3
Train Loss: 0.6991 | Val Loss: 0.6944 | Time: 31.56s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.7454 | Val Loss: 0.7143 | Time: 31.69s
Train Loss: 0.7139 | Val Loss: 0.7147 | Time: 63.10s
Train Loss: 0.7189 | Val Loss: 0.7044 | Time: 94.46s
Train Loss: 0.7065 | Val Loss: 0.7023 | Time: 125.85s
Train Loss: 0.7024 | Val Loss: 0.6990 | Time: 157.20s
Train Loss: 0.6935 | Val Loss: 0.6978 | Time: 188.50s
Train Loss: 0.6957 | Val Loss: 0.6959 | Time: 219.93s
Train Loss: 0.6867 | Val Loss: 0.6980 | Time: 251.30s
Train Loss: 0.6911 | Val Loss: 0.6972 | Time: 282.71s
Train Loss: 0.6935 | Val Loss: 0.6952 | Time: 314.13s
Train Loss: 0.6926 | Val Loss: 0.6921 | Time: 345.55s
Train Loss: 0.6950 | Val Loss: 0.6899 | Time: 376.96s
Train Loss: 0.6978 | Val Loss: 0.6854 | Time: 408.33s
Train Loss: 0.6927 | Val Loss: 0.6831 | Time: 439.77s
Train Loss: 0.6939 | Val Loss: 0.6802 | Time: 471.20s
Train Loss: 0.6944 | Val Loss: 0.6788 | Time: 502.58s
Train Loss: 0.6923 | Val Loss: 0.6784 | Time: 533.90s
Train Loss: 0.6911 | Val Loss: 0.6808 | Time: 565.31s
Train Loss: 0.6892 | Val Loss: 0.6809 | Time: 596.67s
Train Loss: 0.6841 | Val Loss: 0.6835 | Time: 628.00s
Early stopping...

Epoch 2/4
Train Loss: 0.7123 | Val Loss: 0.6851 | Time: 31.50s
Early stopping...

Epoch 3/4
Train Loss: 0.6919 | Val Loss: 0.6872 | Time: 31.78s
Early stopping...

Epoch 4/4
Train Loss: 0.6062 | Val Loss: 0.6888 | Time: 31.61s
Early stopping...

Best Params: batch=32, lr=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.5992 | Val Loss: 0.5842

Epoch 2/2
Train Loss: 0.5647 | Val Loss: 0.5815

Final Evaluation
Accuracy: 0.6694 | Precision: 0.8247 | Recall: 0.4300
F1 Score: 0.5653
Confusion Matrix:
[[1552  156]
 [ 973  734]]
