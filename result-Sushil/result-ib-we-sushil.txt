Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
✅ GPU detected. Training will use GPU.

Running experiment with batch_size=16, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.7202 | Val Loss: 0.7027 | Time: 68.76s
Train Loss: 0.7006 | Val Loss: 0.7015 | Time: 136.33s
Train Loss: 0.7060 | Val Loss: 0.6994 | Time: 202.09s
Train Loss: 0.7050 | Val Loss: 0.6988 | Time: 268.69s
Train Loss: 0.6997 | Val Loss: 0.6988 | Time: 334.90s
Train Loss: 0.6962 | Val Loss: 0.6947 | Time: 401.04s
Train Loss: 0.7016 | Val Loss: 0.6906 | Time: 467.81s
Train Loss: 0.6967 | Val Loss: 0.6903 | Time: 534.47s
Train Loss: 0.6955 | Val Loss: 0.6913 | Time: 600.66s
Train Loss: 0.6978 | Val Loss: 0.6915 | Time: 666.95s
Train Loss: 0.6987 | Val Loss: 0.6914 | Time: 733.58s
Early stopping...

Epoch 2/2
Train Loss: 0.7131 | Val Loss: 0.6899 | Time: 67.09s
Train Loss: 0.7135 | Val Loss: 0.6876 | Time: 133.48s
Train Loss: 0.7219 | Val Loss: 0.6836 | Time: 199.57s
Train Loss: 0.7099 | Val Loss: 0.6797 | Time: 265.76s
Train Loss: 0.7012 | Val Loss: 0.6786 | Time: 332.99s
Train Loss: 0.6979 | Val Loss: 0.6798 | Time: 399.49s
Train Loss: 0.7003 | Val Loss: 0.6817 | Time: 465.06s
Train Loss: 0.7040 | Val Loss: 0.6811 | Time: 534.74s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7228 | Val Loss: 0.6954 | Time: 67.67s
Train Loss: 0.7094 | Val Loss: 0.6908 | Time: 137.18s
Train Loss: 0.6977 | Val Loss: 0.6936 | Time: 203.99s
Train Loss: 0.7040 | Val Loss: 0.6913 | Time: 271.47s
Train Loss: 0.7154 | Val Loss: 0.6862 | Time: 337.55s
Train Loss: 0.7050 | Val Loss: 0.6863 | Time: 403.53s
Train Loss: 0.7046 | Val Loss: 0.6948 | Time: 469.23s
Train Loss: 0.7027 | Val Loss: 0.6958 | Time: 535.03s
Early stopping...

Epoch 2/3
Train Loss: 0.7100 | Val Loss: 0.6964 | Time: 66.72s
Early stopping...

Epoch 3/3
Train Loss: 0.7085 | Val Loss: 0.6994 | Time: 66.43s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.7934 | Val Loss: 0.7014 | Time: 67.49s
Train Loss: 0.7320 | Val Loss: 0.7012 | Time: 133.65s
Train Loss: 0.7045 | Val Loss: 0.7022 | Time: 199.51s
Train Loss: 0.6987 | Val Loss: 0.7050 | Time: 265.61s
Train Loss: 0.7024 | Val Loss: 0.7003 | Time: 331.32s
Train Loss: 0.7142 | Val Loss: 0.6925 | Time: 397.37s
Train Loss: 0.7110 | Val Loss: 0.6889 | Time: 463.15s
Train Loss: 0.7096 | Val Loss: 0.6892 | Time: 528.85s
Train Loss: 0.7081 | Val Loss: 0.6897 | Time: 594.45s
Train Loss: 0.7100 | Val Loss: 0.6935 | Time: 660.04s
Early stopping...

Epoch 2/4
Train Loss: 0.6566 | Val Loss: 0.6957 | Time: 66.00s
Early stopping...

Epoch 3/4
Train Loss: 0.6156 | Val Loss: 0.6963 | Time: 65.94s
Early stopping...

Epoch 4/4
Train Loss: 0.6309 | Val Loss: 0.7013 | Time: 65.87s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.6932 | Val Loss: 0.7000 | Time: 66.09s
Train Loss: 0.6881 | Val Loss: 0.6999 | Time: 132.20s
Train Loss: 0.6932 | Val Loss: 0.7018 | Time: 198.18s
Train Loss: 0.7049 | Val Loss: 0.7022 | Time: 264.46s
Early stopping...

Epoch 2/2
Train Loss: 0.7262 | Val Loss: 0.7002 | Time: 65.77s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.5987 | Val Loss: 0.7491 | Time: 67.26s
Train Loss: 0.6953 | Val Loss: 0.7412 | Time: 134.13s
Train Loss: 0.6651 | Val Loss: 0.7416 | Time: 200.18s
Train Loss: 0.6819 | Val Loss: 0.7375 | Time: 266.34s
Train Loss: 0.6730 | Val Loss: 0.7339 | Time: 332.01s
Train Loss: 0.6957 | Val Loss: 0.7254 | Time: 397.84s
Train Loss: 0.7074 | Val Loss: 0.7172 | Time: 463.47s
Train Loss: 0.7134 | Val Loss: 0.7076 | Time: 529.73s
Train Loss: 0.7125 | Val Loss: 0.7014 | Time: 596.23s
Train Loss: 0.7088 | Val Loss: 0.6968 | Time: 663.38s
Train Loss: 0.7112 | Val Loss: 0.6936 | Time: 729.87s
Train Loss: 0.7112 | Val Loss: 0.6909 | Time: 796.25s
Train Loss: 0.7059 | Val Loss: 0.6895 | Time: 862.60s
Train Loss: 0.7061 | Val Loss: 0.6878 | Time: 929.31s
Train Loss: 0.7052 | Val Loss: 0.6867 | Time: 996.21s
Train Loss: 0.7035 | Val Loss: 0.6861 | Time: 1063.35s
Train Loss: 0.7036 | Val Loss: 0.6847 | Time: 1129.81s
Train Loss: 0.6985 | Val Loss: 0.6831 | Time: 1196.70s
Train Loss: 0.6981 | Val Loss: 0.6804 | Time: 1263.42s
Train Loss: 0.6953 | Val Loss: 0.6790 | Time: 1330.11s
Train Loss: 0.6960 | Val Loss: 0.6768 | Time: 1397.27s
Train Loss: 0.6969 | Val Loss: 0.6761 | Time: 1463.82s
Train Loss: 0.6973 | Val Loss: 0.6756 | Time: 1529.31s
Train Loss: 0.6968 | Val Loss: 0.6749 | Time: 1594.65s
Train Loss: 0.6972 | Val Loss: 0.6706 | Time: 1660.01s
Train Loss: 0.6979 | Val Loss: 0.6692 | Time: 1725.72s
Train Loss: 0.6985 | Val Loss: 0.6705 | Time: 1791.37s
Train Loss: 0.6990 | Val Loss: 0.6690 | Time: 1856.93s
Train Loss: 0.6984 | Val Loss: 0.6691 | Time: 1922.42s
Train Loss: 0.6945 | Val Loss: 0.6694 | Time: 1988.26s
Train Loss: 0.6924 | Val Loss: 0.6701 | Time: 2053.41s
Early stopping...

Epoch 2/3
Train Loss: 0.6308 | Val Loss: 0.6722 | Time: 65.79s
Early stopping...

Epoch 3/3
Train Loss: 0.7736 | Val Loss: 0.6735 | Time: 65.56s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.8342 | Val Loss: 0.6987 | Time: 65.64s
Train Loss: 0.7707 | Val Loss: 0.7020 | Time: 131.69s
Train Loss: 0.7555 | Val Loss: 0.7037 | Time: 197.28s
Train Loss: 0.7434 | Val Loss: 0.7065 | Time: 263.12s
Early stopping...

Epoch 2/4
Train Loss: 0.6965 | Val Loss: 0.7087 | Time: 66.17s
Early stopping...

Epoch 3/4
Train Loss: 0.7439 | Val Loss: 0.7116 | Time: 65.39s
Early stopping...

Epoch 4/4
Train Loss: 0.6331 | Val Loss: 0.7128 | Time: 66.40s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.7296 | Val Loss: 0.6937 | Time: 66.37s
Train Loss: 0.7012 | Val Loss: 0.6915 | Time: 133.07s
Train Loss: 0.6892 | Val Loss: 0.6900 | Time: 199.51s
Train Loss: 0.6917 | Val Loss: 0.6898 | Time: 265.72s
Train Loss: 0.6863 | Val Loss: 0.6873 | Time: 331.94s
Train Loss: 0.6921 | Val Loss: 0.6867 | Time: 397.40s
Train Loss: 0.6885 | Val Loss: 0.6853 | Time: 463.61s
Train Loss: 0.6961 | Val Loss: 0.6843 | Time: 529.48s
Train Loss: 0.6935 | Val Loss: 0.6841 | Time: 595.33s
Train Loss: 0.6919 | Val Loss: 0.6838 | Time: 662.43s
Train Loss: 0.6894 | Val Loss: 0.6840 | Time: 727.99s
Train Loss: 0.6906 | Val Loss: 0.6840 | Time: 793.26s
Train Loss: 0.6871 | Val Loss: 0.6843 | Time: 858.99s
Early stopping...

Epoch 2/2
Train Loss: 0.6747 | Val Loss: 0.6847 | Time: 66.31s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.8301 | Val Loss: 0.7162 | Time: 66.15s
Train Loss: 0.8078 | Val Loss: 0.7080 | Time: 131.47s
Train Loss: 0.7701 | Val Loss: 0.7028 | Time: 197.62s
Train Loss: 0.7595 | Val Loss: 0.7005 | Time: 264.57s
Train Loss: 0.7409 | Val Loss: 0.6989 | Time: 331.17s
Train Loss: 0.7253 | Val Loss: 0.6960 | Time: 397.44s
Train Loss: 0.7142 | Val Loss: 0.6926 | Time: 463.29s
Train Loss: 0.7148 | Val Loss: 0.6913 | Time: 529.34s
Train Loss: 0.7070 | Val Loss: 0.6911 | Time: 595.88s
Train Loss: 0.6961 | Val Loss: 0.6925 | Time: 661.68s
Train Loss: 0.6880 | Val Loss: 0.6931 | Time: 727.41s
Train Loss: 0.6893 | Val Loss: 0.6940 | Time: 792.73s
Early stopping...

Epoch 2/3
Train Loss: 0.6283 | Val Loss: 0.6951 | Time: 65.64s
Early stopping...

Epoch 3/3
Train Loss: 0.7782 | Val Loss: 0.6957 | Time: 66.00s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.6759 | Val Loss: 0.7111 | Time: 65.90s
Train Loss: 0.6919 | Val Loss: 0.7121 | Time: 131.91s
Train Loss: 0.6692 | Val Loss: 0.7156 | Time: 197.46s
Train Loss: 0.6891 | Val Loss: 0.7149 | Time: 263.06s
Early stopping...

Epoch 2/4
Train Loss: 0.7306 | Val Loss: 0.7117 | Time: 65.87s
Early stopping...

Epoch 3/4
Train Loss: 0.7270 | Val Loss: 0.7074 | Time: 66.46s
Train Loss: 0.7546 | Val Loss: 0.7023 | Time: 132.47s
Train Loss: 0.7529 | Val Loss: 0.6982 | Time: 197.89s
Train Loss: 0.7373 | Val Loss: 0.6951 | Time: 263.86s
Train Loss: 0.7299 | Val Loss: 0.6927 | Time: 329.87s
Train Loss: 0.7254 | Val Loss: 0.6894 | Time: 395.31s
Train Loss: 0.7245 | Val Loss: 0.6863 | Time: 460.59s
Train Loss: 0.7135 | Val Loss: 0.6831 | Time: 526.33s
Train Loss: 0.7146 | Val Loss: 0.6821 | Time: 592.09s
Train Loss: 0.7062 | Val Loss: 0.6815 | Time: 658.25s
Train Loss: 0.7064 | Val Loss: 0.6810 | Time: 724.44s
Train Loss: 0.7058 | Val Loss: 0.6800 | Time: 790.27s
Train Loss: 0.7079 | Val Loss: 0.6790 | Time: 856.04s
Train Loss: 0.7051 | Val Loss: 0.6787 | Time: 922.07s
Train Loss: 0.7057 | Val Loss: 0.6784 | Time: 988.30s
Train Loss: 0.7023 | Val Loss: 0.6784 | Time: 1056.20s
Train Loss: 0.7039 | Val Loss: 0.6777 | Time: 1124.16s
Train Loss: 0.6992 | Val Loss: 0.6767 | Time: 1190.78s
Train Loss: 0.7039 | Val Loss: 0.6762 | Time: 1257.33s
Train Loss: 0.7046 | Val Loss: 0.6762 | Time: 1323.39s
Train Loss: 0.7019 | Val Loss: 0.6757 | Time: 1389.43s
Train Loss: 0.7006 | Val Loss: 0.6750 | Time: 1455.25s
Train Loss: 0.6986 | Val Loss: 0.6749 | Time: 1520.39s
Train Loss: 0.7002 | Val Loss: 0.6752 | Time: 1585.74s
Train Loss: 0.6985 | Val Loss: 0.6754 | Time: 1651.56s
Train Loss: 0.6979 | Val Loss: 0.6749 | Time: 1716.68s
Early stopping...

Epoch 4/4
Train Loss: 0.7102 | Val Loss: 0.6745 | Time: 66.02s
Train Loss: 0.7280 | Val Loss: 0.6743 | Time: 131.95s
Train Loss: 0.6961 | Val Loss: 0.6749 | Time: 197.99s
Train Loss: 0.6875 | Val Loss: 0.6745 | Time: 264.73s
Train Loss: 0.7045 | Val Loss: 0.6734 | Time: 330.54s
Train Loss: 0.6887 | Val Loss: 0.6727 | Time: 396.27s
Train Loss: 0.6843 | Val Loss: 0.6719 | Time: 462.21s
Train Loss: 0.6806 | Val Loss: 0.6713 | Time: 527.81s
Train Loss: 0.6906 | Val Loss: 0.6707 | Time: 593.45s
Train Loss: 0.6970 | Val Loss: 0.6700 | Time: 659.11s
Train Loss: 0.6946 | Val Loss: 0.6691 | Time: 724.90s
Train Loss: 0.6889 | Val Loss: 0.6689 | Time: 790.60s
Train Loss: 0.6868 | Val Loss: 0.6684 | Time: 856.11s
Train Loss: 0.6830 | Val Loss: 0.6682 | Time: 922.24s
Train Loss: 0.6861 | Val Loss: 0.6679 | Time: 987.74s
Train Loss: 0.6883 | Val Loss: 0.6681 | Time: 1053.31s
Train Loss: 0.6923 | Val Loss: 0.6681 | Time: 1119.08s
Train Loss: 0.6920 | Val Loss: 0.6685 | Time: 1184.45s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.7058 | Val Loss: 0.6969 | Time: 38.89s
Train Loss: 0.7026 | Val Loss: 0.6950 | Time: 77.62s
Train Loss: 0.6952 | Val Loss: 0.6922 | Time: 116.03s
Train Loss: 0.6968 | Val Loss: 0.6917 | Time: 153.69s
Train Loss: 0.7043 | Val Loss: 0.6896 | Time: 190.54s
Train Loss: 0.7022 | Val Loss: 0.6884 | Time: 227.17s
Train Loss: 0.7119 | Val Loss: 0.6813 | Time: 263.95s
Train Loss: 0.7090 | Val Loss: 0.6788 | Time: 302.60s
Train Loss: 0.7067 | Val Loss: 0.6762 | Time: 341.19s
Train Loss: 0.7062 | Val Loss: 0.6757 | Time: 378.96s
Train Loss: 0.7020 | Val Loss: 0.6743 | Time: 415.56s
Train Loss: 0.6985 | Val Loss: 0.6757 | Time: 453.12s
Train Loss: 0.6989 | Val Loss: 0.6753 | Time: 489.72s
Train Loss: 0.6985 | Val Loss: 0.6721 | Time: 526.26s
Train Loss: 0.6960 | Val Loss: 0.6700 | Time: 563.16s
Train Loss: 0.6933 | Val Loss: 0.6668 | Time: 600.11s
Train Loss: 0.6915 | Val Loss: 0.6613 | Time: 656.17s
Train Loss: 0.6879 | Val Loss: 0.6593 | Time: 723.95s
Train Loss: 0.6829 | Val Loss: 0.6584 | Time: 794.44s
Train Loss: 0.6839 | Val Loss: 0.6601 | Time: 863.75s
Train Loss: 0.6825 | Val Loss: 0.6577 | Time: 934.72s
Train Loss: 0.6815 | Val Loss: 0.6668 | Time: 1000.85s
Train Loss: 0.6800 | Val Loss: 0.6680 | Time: 1067.20s
Train Loss: 0.6791 | Val Loss: 0.6751 | Time: 1136.49s
Early stopping...

Epoch 2/2
Train Loss: 0.7221 | Val Loss: 0.6780 | Time: 74.50s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7249 | Val Loss: 0.6910 | Time: 69.50s
Train Loss: 0.7286 | Val Loss: 0.6836 | Time: 153.13s
Train Loss: 0.7081 | Val Loss: 0.6847 | Time: 222.38s
Train Loss: 0.7083 | Val Loss: 0.6856 | Time: 292.17s
Train Loss: 0.7047 | Val Loss: 0.6843 | Time: 365.16s
Early stopping...

Epoch 2/3
Train Loss: 0.6511 | Val Loss: 0.6874 | Time: 71.51s
Early stopping...

Epoch 3/3
Train Loss: 0.6789 | Val Loss: 0.6896 | Time: 73.27s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.6609 | Val Loss: 0.7247 | Time: 85.03s
Train Loss: 0.6900 | Val Loss: 0.7069 | Time: 160.69s
Train Loss: 0.6771 | Val Loss: 0.7028 | Time: 245.85s
Train Loss: 0.6719 | Val Loss: 0.6990 | Time: 315.00s
Train Loss: 0.6785 | Val Loss: 0.6970 | Time: 386.47s
Train Loss: 0.6768 | Val Loss: 0.6986 | Time: 457.86s
Train Loss: 0.6854 | Val Loss: 0.6958 | Time: 531.88s
Train Loss: 0.6849 | Val Loss: 0.6964 | Time: 615.96s
Train Loss: 0.6826 | Val Loss: 0.6939 | Time: 662.62s
Train Loss: 0.6817 | Val Loss: 0.6952 | Time: 701.63s
Train Loss: 0.6839 | Val Loss: 0.6920 | Time: 740.74s
Train Loss: 0.6922 | Val Loss: 0.6958 | Time: 777.91s
Train Loss: 0.6947 | Val Loss: 0.6993 | Time: 816.75s
Train Loss: 0.6960 | Val Loss: 0.6941 | Time: 854.40s
Early stopping...

Epoch 2/4
Train Loss: 0.7316 | Val Loss: 0.6884 | Time: 38.35s
Train Loss: 0.6923 | Val Loss: 0.6853 | Time: 75.90s
Train Loss: 0.7047 | Val Loss: 0.6831 | Time: 113.29s
Train Loss: 0.6965 | Val Loss: 0.6796 | Time: 150.11s
Train Loss: 0.6971 | Val Loss: 0.6752 | Time: 188.22s
Train Loss: 0.6891 | Val Loss: 0.6707 | Time: 225.64s
Train Loss: 0.6883 | Val Loss: 0.6690 | Time: 263.33s
Train Loss: 0.6839 | Val Loss: 0.6681 | Time: 301.18s
Train Loss: 0.6831 | Val Loss: 0.6676 | Time: 338.82s
Train Loss: 0.6840 | Val Loss: 0.6661 | Time: 377.16s
Train Loss: 0.6882 | Val Loss: 0.6654 | Time: 415.41s
Train Loss: 0.6868 | Val Loss: 0.6644 | Time: 453.23s
Train Loss: 0.6872 | Val Loss: 0.6629 | Time: 491.18s
Train Loss: 0.6829 | Val Loss: 0.6613 | Time: 529.02s
Train Loss: 0.6813 | Val Loss: 0.6616 | Time: 566.62s
Train Loss: 0.6803 | Val Loss: 0.6623 | Time: 604.25s
Train Loss: 0.6812 | Val Loss: 0.6635 | Time: 642.71s
Early stopping...

Epoch 3/4
Train Loss: 0.6612 | Val Loss: 0.6647 | Time: 38.81s
Early stopping...

Epoch 4/4
Train Loss: 0.6110 | Val Loss: 0.6657 | Time: 38.89s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.7100 | Val Loss: 0.6981 | Time: 37.31s
Train Loss: 0.7188 | Val Loss: 0.6928 | Time: 74.25s
Train Loss: 0.7119 | Val Loss: 0.6917 | Time: 112.29s
Train Loss: 0.7055 | Val Loss: 0.6903 | Time: 149.50s
Train Loss: 0.7075 | Val Loss: 0.6868 | Time: 186.70s
Train Loss: 0.7012 | Val Loss: 0.6823 | Time: 223.35s
Train Loss: 0.6951 | Val Loss: 0.6800 | Time: 261.16s
Train Loss: 0.6987 | Val Loss: 0.6785 | Time: 298.78s
Train Loss: 0.6964 | Val Loss: 0.6770 | Time: 335.30s
Train Loss: 0.6957 | Val Loss: 0.6753 | Time: 371.88s
Train Loss: 0.7004 | Val Loss: 0.6735 | Time: 409.75s
Train Loss: 0.6985 | Val Loss: 0.6721 | Time: 446.34s
Train Loss: 0.6956 | Val Loss: 0.6706 | Time: 485.18s
Train Loss: 0.6906 | Val Loss: 0.6693 | Time: 523.26s
Train Loss: 0.6916 | Val Loss: 0.6683 | Time: 560.18s
Train Loss: 0.6906 | Val Loss: 0.6673 | Time: 597.55s
Train Loss: 0.6886 | Val Loss: 0.6668 | Time: 634.87s
Train Loss: 0.6879 | Val Loss: 0.6662 | Time: 672.67s
Train Loss: 0.6876 | Val Loss: 0.6656 | Time: 711.17s
Train Loss: 0.6869 | Val Loss: 0.6655 | Time: 749.43s
Train Loss: 0.6858 | Val Loss: 0.6650 | Time: 785.98s
Train Loss: 0.6855 | Val Loss: 0.6647 | Time: 822.87s
Train Loss: 0.6870 | Val Loss: 0.6650 | Time: 860.20s
Train Loss: 0.6861 | Val Loss: 0.6636 | Time: 896.88s
Train Loss: 0.6850 | Val Loss: 0.6612 | Time: 933.46s
Train Loss: 0.6839 | Val Loss: 0.6588 | Time: 971.58s
Train Loss: 0.6838 | Val Loss: 0.6558 | Time: 1008.93s
Train Loss: 0.6823 | Val Loss: 0.6529 | Time: 1046.19s
Train Loss: 0.6831 | Val Loss: 0.6509 | Time: 1084.69s
Train Loss: 0.6811 | Val Loss: 0.6498 | Time: 1123.25s
Train Loss: 0.6798 | Val Loss: 0.6491 | Time: 1161.75s
Train Loss: 0.6811 | Val Loss: 0.6480 | Time: 1199.85s
Train Loss: 0.6790 | Val Loss: 0.6475 | Time: 1236.49s
Train Loss: 0.6784 | Val Loss: 0.6471 | Time: 1273.07s
Train Loss: 0.6776 | Val Loss: 0.6469 | Time: 1310.64s
Train Loss: 0.6761 | Val Loss: 0.6469 | Time: 1348.00s
Train Loss: 0.6752 | Val Loss: 0.6462 | Time: 1385.09s
Train Loss: 0.6729 | Val Loss: 0.6444 | Time: 1423.17s
Train Loss: 0.6728 | Val Loss: 0.6413 | Time: 1461.11s
Train Loss: 0.6711 | Val Loss: 0.6406 | Time: 1498.41s
Train Loss: 0.6701 | Val Loss: 0.6395 | Time: 1535.04s
Train Loss: 0.6705 | Val Loss: 0.6354 | Time: 1572.51s
Train Loss: 0.6703 | Val Loss: 0.6335 | Time: 1615.39s
Train Loss: 0.6701 | Val Loss: 0.6332 | Time: 1687.15s
Train Loss: 0.6674 | Val Loss: 0.6320 | Time: 1764.19s
Train Loss: 0.6657 | Val Loss: 0.6287 | Time: 1838.88s
Train Loss: 0.6643 | Val Loss: 0.6254 | Time: 1912.55s
Train Loss: 0.6654 | Val Loss: 0.6230 | Time: 1981.43s
Train Loss: 0.6635 | Val Loss: 0.6212 | Time: 2051.24s
Train Loss: 0.6611 | Val Loss: 0.6194 | Time: 2124.47s
Train Loss: 0.6597 | Val Loss: 0.6187 | Time: 2209.14s
Train Loss: 0.6585 | Val Loss: 0.6192 | Time: 2277.70s
Train Loss: 0.6595 | Val Loss: 0.6223 | Time: 2347.32s
Train Loss: 0.6599 | Val Loss: 0.6237 | Time: 2417.42s
Early stopping...

Epoch 2/2
Train Loss: 0.7516 | Val Loss: 0.6192 | Time: 72.64s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7011 | Val Loss: 0.7031 | Time: 72.23s
Train Loss: 0.7059 | Val Loss: 0.6999 | Time: 142.65s
Train Loss: 0.7138 | Val Loss: 0.6945 | Time: 211.18s
Train Loss: 0.7165 | Val Loss: 0.6903 | Time: 282.63s
Train Loss: 0.7131 | Val Loss: 0.6878 | Time: 355.26s
Train Loss: 0.7035 | Val Loss: 0.6858 | Time: 427.94s
Train Loss: 0.7007 | Val Loss: 0.6844 | Time: 498.61s
Train Loss: 0.6997 | Val Loss: 0.6815 | Time: 570.10s
Train Loss: 0.6987 | Val Loss: 0.6797 | Time: 639.98s
Train Loss: 0.7011 | Val Loss: 0.6780 | Time: 710.11s
Train Loss: 0.6981 | Val Loss: 0.6770 | Time: 793.76s
Train Loss: 0.7021 | Val Loss: 0.6767 | Time: 864.69s
Train Loss: 0.7035 | Val Loss: 0.6769 | Time: 935.78s
Train Loss: 0.7000 | Val Loss: 0.6775 | Time: 1009.44s
Train Loss: 0.6993 | Val Loss: 0.6764 | Time: 1083.51s
Train Loss: 0.6989 | Val Loss: 0.6753 | Time: 1152.10s
Train Loss: 0.6993 | Val Loss: 0.6735 | Time: 1224.23s
Train Loss: 0.6990 | Val Loss: 0.6727 | Time: 1295.36s
Train Loss: 0.7002 | Val Loss: 0.6719 | Time: 1363.13s
Train Loss: 0.6990 | Val Loss: 0.6710 | Time: 1435.28s
Train Loss: 0.6956 | Val Loss: 0.6697 | Time: 1503.57s
Train Loss: 0.6939 | Val Loss: 0.6698 | Time: 1574.16s
Train Loss: 0.6919 | Val Loss: 0.6707 | Time: 1646.02s
Train Loss: 0.6905 | Val Loss: 0.6727 | Time: 1720.84s
Early stopping...

Epoch 2/3
Train Loss: 0.7056 | Val Loss: 0.6746 | Time: 73.04s
Early stopping...

Epoch 3/3
Train Loss: 0.6762 | Val Loss: 0.6750 | Time: 71.96s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.7173 | Val Loss: 0.7219 | Time: 76.30s
Train Loss: 0.7024 | Val Loss: 0.7070 | Time: 150.67s
Train Loss: 0.7069 | Val Loss: 0.6992 | Time: 223.63s
Train Loss: 0.7221 | Val Loss: 0.6866 | Time: 297.02s
Train Loss: 0.7143 | Val Loss: 0.6793 | Time: 380.60s
Train Loss: 0.7137 | Val Loss: 0.6771 | Time: 454.34s
Train Loss: 0.7071 | Val Loss: 0.6742 | Time: 523.55s
Train Loss: 0.7031 | Val Loss: 0.6724 | Time: 590.07s
Train Loss: 0.7024 | Val Loss: 0.6726 | Time: 660.83s
Train Loss: 0.6959 | Val Loss: 0.6719 | Time: 730.90s
Train Loss: 0.6959 | Val Loss: 0.6696 | Time: 800.59s
Train Loss: 0.6898 | Val Loss: 0.6673 | Time: 884.55s
Train Loss: 0.6883 | Val Loss: 0.6640 | Time: 952.39s
Train Loss: 0.6848 | Val Loss: 0.6603 | Time: 1022.16s
Train Loss: 0.6835 | Val Loss: 0.6586 | Time: 1089.51s
Train Loss: 0.6826 | Val Loss: 0.6564 | Time: 1158.79s
Train Loss: 0.6772 | Val Loss: 0.6519 | Time: 1223.48s
Train Loss: 0.6746 | Val Loss: 0.6477 | Time: 1306.63s
Train Loss: 0.6733 | Val Loss: 0.6443 | Time: 1370.09s
Train Loss: 0.6720 | Val Loss: 0.6418 | Time: 1434.78s
Train Loss: 0.6682 | Val Loss: 0.6393 | Time: 1498.47s
Train Loss: 0.6697 | Val Loss: 0.6354 | Time: 1561.28s
Train Loss: 0.6680 | Val Loss: 0.6324 | Time: 1626.61s
Train Loss: 0.6701 | Val Loss: 0.6310 | Time: 1690.57s
Train Loss: 0.6709 | Val Loss: 0.6300 | Time: 1756.01s
Train Loss: 0.6713 | Val Loss: 0.6296 | Time: 1820.55s
Train Loss: 0.6683 | Val Loss: 0.6293 | Time: 1904.96s
Train Loss: 0.6648 | Val Loss: 0.6296 | Time: 1988.58s
Train Loss: 0.6665 | Val Loss: 0.6314 | Time: 2053.09s
Train Loss: 0.6649 | Val Loss: 0.6339 | Time: 2119.14s
Early stopping...

Epoch 2/4
Train Loss: 0.5685 | Val Loss: 0.6341 | Time: 84.66s
Early stopping...

Epoch 3/4
Train Loss: 0.5767 | Val Loss: 0.6327 | Time: 64.84s
Early stopping...

Epoch 4/4
Train Loss: 0.7059 | Val Loss: 0.6317 | Time: 66.01s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.6744 | Val Loss: 0.6990 | Time: 62.93s
Train Loss: 0.7219 | Val Loss: 0.6974 | Time: 127.67s
Train Loss: 0.7079 | Val Loss: 0.6967 | Time: 193.34s
Train Loss: 0.7054 | Val Loss: 0.6954 | Time: 258.72s
Train Loss: 0.7063 | Val Loss: 0.6941 | Time: 324.05s
Train Loss: 0.7025 | Val Loss: 0.6930 | Time: 386.94s
Train Loss: 0.7059 | Val Loss: 0.6911 | Time: 451.06s
Train Loss: 0.7053 | Val Loss: 0.6905 | Time: 516.27s
Train Loss: 0.7092 | Val Loss: 0.6899 | Time: 579.85s
Train Loss: 0.7061 | Val Loss: 0.6893 | Time: 664.41s
Train Loss: 0.7059 | Val Loss: 0.6888 | Time: 728.00s
Train Loss: 0.7031 | Val Loss: 0.6878 | Time: 796.69s
Train Loss: 0.7020 | Val Loss: 0.6872 | Time: 862.92s
Train Loss: 0.6998 | Val Loss: 0.6861 | Time: 925.70s
Train Loss: 0.7007 | Val Loss: 0.6852 | Time: 991.21s
Train Loss: 0.6992 | Val Loss: 0.6844 | Time: 1055.79s
Train Loss: 0.6978 | Val Loss: 0.6821 | Time: 1124.12s
Train Loss: 0.6954 | Val Loss: 0.6803 | Time: 1188.56s
Train Loss: 0.6967 | Val Loss: 0.6789 | Time: 1252.88s
Train Loss: 0.6951 | Val Loss: 0.6775 | Time: 1320.09s
Train Loss: 0.6945 | Val Loss: 0.6763 | Time: 1382.92s
Train Loss: 0.6948 | Val Loss: 0.6757 | Time: 1449.12s
Train Loss: 0.6947 | Val Loss: 0.6753 | Time: 1513.96s
Train Loss: 0.6938 | Val Loss: 0.6746 | Time: 1577.32s
Train Loss: 0.6916 | Val Loss: 0.6738 | Time: 1644.07s
Train Loss: 0.6918 | Val Loss: 0.6730 | Time: 1710.75s
Train Loss: 0.6923 | Val Loss: 0.6723 | Time: 1777.66s
Train Loss: 0.6907 | Val Loss: 0.6716 | Time: 1843.78s
Train Loss: 0.6894 | Val Loss: 0.6706 | Time: 1908.11s
Train Loss: 0.6891 | Val Loss: 0.6701 | Time: 1976.89s
Train Loss: 0.6864 | Val Loss: 0.6706 | Time: 2043.02s
Train Loss: 0.6861 | Val Loss: 0.6711 | Time: 2106.96s
Train Loss: 0.6849 | Val Loss: 0.6705 | Time: 2173.62s
Early stopping...

Epoch 2/2
Train Loss: 0.6748 | Val Loss: 0.6700 | Time: 68.01s
Train Loss: 0.6834 | Val Loss: 0.6696 | Time: 135.08s
Train Loss: 0.6610 | Val Loss: 0.6685 | Time: 201.83s
Train Loss: 0.6693 | Val Loss: 0.6670 | Time: 268.24s
Train Loss: 0.6552 | Val Loss: 0.6665 | Time: 335.86s
Train Loss: 0.6602 | Val Loss: 0.6657 | Time: 403.76s
Train Loss: 0.6632 | Val Loss: 0.6640 | Time: 470.86s
Train Loss: 0.6784 | Val Loss: 0.6630 | Time: 538.75s
Train Loss: 0.6777 | Val Loss: 0.6624 | Time: 604.74s
Train Loss: 0.6753 | Val Loss: 0.6622 | Time: 671.28s
Train Loss: 0.6751 | Val Loss: 0.6624 | Time: 737.49s
Train Loss: 0.6753 | Val Loss: 0.6630 | Time: 822.24s
Train Loss: 0.6741 | Val Loss: 0.6642 | Time: 887.15s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.7069 | Val Loss: 0.6926 | Time: 68.89s
Train Loss: 0.7005 | Val Loss: 0.6898 | Time: 138.08s
Train Loss: 0.6955 | Val Loss: 0.6884 | Time: 209.29s
Train Loss: 0.6969 | Val Loss: 0.6888 | Time: 274.72s
Train Loss: 0.6917 | Val Loss: 0.6879 | Time: 341.34s
Train Loss: 0.6898 | Val Loss: 0.6848 | Time: 414.28s
Train Loss: 0.6899 | Val Loss: 0.6839 | Time: 481.54s
Train Loss: 0.6903 | Val Loss: 0.6832 | Time: 548.27s
Train Loss: 0.6904 | Val Loss: 0.6818 | Time: 614.07s
Train Loss: 0.6937 | Val Loss: 0.6803 | Time: 680.09s
Train Loss: 0.6928 | Val Loss: 0.6786 | Time: 745.03s
Train Loss: 0.6961 | Val Loss: 0.6773 | Time: 809.11s
Train Loss: 0.6951 | Val Loss: 0.6758 | Time: 876.15s
Train Loss: 0.6934 | Val Loss: 0.6746 | Time: 959.30s
Train Loss: 0.6901 | Val Loss: 0.6736 | Time: 1025.03s
Train Loss: 0.6864 | Val Loss: 0.6728 | Time: 1090.43s
Train Loss: 0.6863 | Val Loss: 0.6722 | Time: 1157.12s
Train Loss: 0.6859 | Val Loss: 0.6715 | Time: 1221.63s
Train Loss: 0.6852 | Val Loss: 0.6715 | Time: 1305.10s
Train Loss: 0.6860 | Val Loss: 0.6704 | Time: 1368.68s
Train Loss: 0.6852 | Val Loss: 0.6692 | Time: 1435.68s
Train Loss: 0.6846 | Val Loss: 0.6678 | Time: 1501.53s
Train Loss: 0.6857 | Val Loss: 0.6674 | Time: 1566.60s
Train Loss: 0.6849 | Val Loss: 0.6674 | Time: 1633.79s
Train Loss: 0.6848 | Val Loss: 0.6678 | Time: 1700.34s
Train Loss: 0.6830 | Val Loss: 0.6685 | Time: 1784.82s
Early stopping...

Epoch 2/3
Train Loss: 0.6493 | Val Loss: 0.6702 | Time: 64.92s
Early stopping...

Epoch 3/3
Train Loss: 0.6909 | Val Loss: 0.6728 | Time: 63.54s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.7890 | Val Loss: 0.6964 | Time: 68.78s
Train Loss: 0.7740 | Val Loss: 0.6954 | Time: 134.58s
Train Loss: 0.7405 | Val Loss: 0.6940 | Time: 201.93s
Train Loss: 0.7330 | Val Loss: 0.6920 | Time: 268.23s
Train Loss: 0.7214 | Val Loss: 0.6906 | Time: 351.45s
Train Loss: 0.7186 | Val Loss: 0.6888 | Time: 418.71s
Train Loss: 0.7158 | Val Loss: 0.6884 | Time: 487.52s
Train Loss: 0.7089 | Val Loss: 0.6892 | Time: 556.80s
Train Loss: 0.7039 | Val Loss: 0.6894 | Time: 639.62s
Train Loss: 0.7021 | Val Loss: 0.6900 | Time: 708.57s
Early stopping...

Epoch 2/4
Train Loss: 0.7571 | Val Loss: 0.6888 | Time: 68.93s
Early stopping...

Epoch 3/4
Train Loss: 0.6822 | Val Loss: 0.6866 | Time: 69.26s
Train Loss: 0.6872 | Val Loss: 0.6849 | Time: 138.16s
Train Loss: 0.6965 | Val Loss: 0.6838 | Time: 206.34s
Train Loss: 0.6948 | Val Loss: 0.6836 | Time: 273.46s
Train Loss: 0.6999 | Val Loss: 0.6824 | Time: 340.19s
Train Loss: 0.7036 | Val Loss: 0.6821 | Time: 407.34s
Train Loss: 0.6996 | Val Loss: 0.6816 | Time: 475.45s
Train Loss: 0.6963 | Val Loss: 0.6822 | Time: 546.88s
Train Loss: 0.7007 | Val Loss: 0.6830 | Time: 614.17s
Train Loss: 0.6978 | Val Loss: 0.6823 | Time: 681.13s
Early stopping...

Epoch 4/4
Train Loss: 0.6729 | Val Loss: 0.6813 | Time: 66.91s
Train Loss: 0.6699 | Val Loss: 0.6803 | Time: 132.89s
Train Loss: 0.6679 | Val Loss: 0.6796 | Time: 216.68s
Train Loss: 0.6659 | Val Loss: 0.6789 | Time: 300.79s
Train Loss: 0.6729 | Val Loss: 0.6773 | Time: 366.13s
Train Loss: 0.6670 | Val Loss: 0.6769 | Time: 431.64s
Train Loss: 0.6719 | Val Loss: 0.6769 | Time: 497.65s
Train Loss: 0.6765 | Val Loss: 0.6763 | Time: 565.44s
Train Loss: 0.6791 | Val Loss: 0.6765 | Time: 632.21s
Train Loss: 0.6819 | Val Loss: 0.6766 | Time: 697.30s
Train Loss: 0.6805 | Val Loss: 0.6772 | Time: 781.02s
Early stopping...

Best Params: batch=32, lr=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.5476 | Val Loss: 0.5097

Epoch 2/2
Train Loss: 0.4900 | Val Loss: 0.4934

Final Evaluation
Accuracy: 0.7408 | Precision: 0.8104 | Recall: 0.6286
F1 Score: 0.7080
Confusion Matrix:
[[1433  247]
 [ 624 1056]]
