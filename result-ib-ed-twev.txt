2025-04-26 11:18:43.669704: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
⚠️ GPU not found. Training will run on CPU.

Running experiment with batch_size=16, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.6885 | Val Loss: 0.6736 | Time: 222.36s
Train Loss: 0.6844 | Val Loss: 0.6704 | Time: 441.85s
Train Loss: 0.6979 | Val Loss: 0.6347 | Time: 668.56s
Train Loss: 0.6885 | Val Loss: 0.5884 | Time: 915.60s
Train Loss: 0.6916 | Val Loss: 0.5794 | Time: 1144.88s
Train Loss: 0.6878 | Val Loss: 0.5869 | Time: 1372.59s
Train Loss: 0.6813 | Val Loss: 0.5785 | Time: 1601.67s
Train Loss: 0.6836 | Val Loss: 0.5690 | Time: 1844.45s
Train Loss: 0.6768 | Val Loss: 0.5730 | Time: 2066.50s
Train Loss: 0.6840 | Val Loss: 0.6286 | Time: 2293.15s
Train Loss: 0.6892 | Val Loss: 0.6683 | Time: 2503.71s
Early stopping...

Epoch 2/2
Train Loss: 0.7142 | Val Loss: 0.6674 | Time: 215.45s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7368 | Val Loss: 0.7073 | Time: 216.57s
Train Loss: 0.7205 | Val Loss: 0.6659 | Time: 431.84s
Train Loss: 0.7202 | Val Loss: 0.7028 | Time: 643.77s
Train Loss: 0.7240 | Val Loss: 0.7160 | Time: 851.42s
Train Loss: 0.7225 | Val Loss: 0.6891 | Time: 1065.70s
Early stopping...

Epoch 2/3
Train Loss: 0.7196 | Val Loss: 0.7143 | Time: 215.44s
Early stopping...

Epoch 3/3
Train Loss: 0.6872 | Val Loss: 0.6821 | Time: 215.40s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.6882 | Val Loss: 0.7172 | Time: 243.87s
Train Loss: 0.6997 | Val Loss: 0.7298 | Time: 460.00s
Train Loss: 0.7067 | Val Loss: 0.6987 | Time: 673.63s
Train Loss: 0.7202 | Val Loss: 0.6831 | Time: 894.59s
Train Loss: 0.7112 | Val Loss: 0.6793 | Time: 1110.15s
Train Loss: 0.7077 | Val Loss: 0.6747 | Time: 1331.22s
Train Loss: 0.6974 | Val Loss: 0.6808 | Time: 1558.97s
Train Loss: 0.6989 | Val Loss: 0.6667 | Time: 1780.82s
Train Loss: 0.6974 | Val Loss: 0.6458 | Time: 1999.59s
Train Loss: 0.6975 | Val Loss: 0.6370 | Time: 2237.90s
Train Loss: 0.6922 | Val Loss: 0.6354 | Time: 2469.05s
Train Loss: 0.6968 | Val Loss: 0.7189 | Time: 2688.43s
Train Loss: 0.6929 | Val Loss: 0.7301 | Time: 2911.50s
Train Loss: 0.6993 | Val Loss: 0.7077 | Time: 3128.11s
Early stopping...

Epoch 2/4
Train Loss: 0.7064 | Val Loss: 0.6836 | Time: 216.81s
Early stopping...

Epoch 3/4
Train Loss: 0.6215 | Val Loss: 0.6826 | Time: 216.42s
Early stopping...

Epoch 4/4
Train Loss: 0.6762 | Val Loss: 0.6561 | Time: 219.54s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.8851 | Val Loss: 0.7253 | Time: 223.10s
Train Loss: 0.8141 | Val Loss: 0.7023 | Time: 459.66s
Train Loss: 0.7817 | Val Loss: 0.7063 | Time: 674.18s
Train Loss: 0.7650 | Val Loss: 0.6930 | Time: 891.62s
Train Loss: 0.7465 | Val Loss: 0.6827 | Time: 1111.13s
Train Loss: 0.7324 | Val Loss: 0.6793 | Time: 1334.51s
Train Loss: 0.7289 | Val Loss: 0.6774 | Time: 1547.42s
Train Loss: 0.7236 | Val Loss: 0.6754 | Time: 1785.18s
Train Loss: 0.7218 | Val Loss: 0.6494 | Time: 1998.29s
Train Loss: 0.7203 | Val Loss: 0.6234 | Time: 2222.87s
Train Loss: 0.7303 | Val Loss: 0.6405 | Time: 2449.28s
Train Loss: 0.7253 | Val Loss: 0.6582 | Time: 2687.00s
Train Loss: 0.7238 | Val Loss: 0.6580 | Time: 2901.32s
Early stopping...

Epoch 2/2
Train Loss: 0.7003 | Val Loss: 0.6468 | Time: 242.38s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7320 | Val Loss: 0.6929 | Time: 204.38s
Train Loss: 0.7337 | Val Loss: 0.6707 | Time: 413.33s
Train Loss: 0.7192 | Val Loss: 0.6279 | Time: 626.64s
Train Loss: 0.7165 | Val Loss: 0.6860 | Time: 838.03s
Train Loss: 0.6973 | Val Loss: 0.6745 | Time: 1075.55s
Train Loss: 0.6911 | Val Loss: 0.6607 | Time: 1299.30s
Early stopping...

Epoch 2/3
Train Loss: 0.6659 | Val Loss: 0.6553 | Time: 219.28s
Early stopping...

Epoch 3/3
Train Loss: 0.7102 | Val Loss: 0.6862 | Time: 208.74s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.7503 | Val Loss: 0.6987 | Time: 215.90s
Train Loss: 0.7356 | Val Loss: 0.6536 | Time: 437.95s
Train Loss: 0.7220 | Val Loss: 0.6770 | Time: 640.91s
Train Loss: 0.7151 | Val Loss: 0.6544 | Time: 789.77s
Train Loss: 0.7123 | Val Loss: 0.6624 | Time: 926.47s
Early stopping...

Epoch 2/4
Train Loss: 0.6536 | Val Loss: 0.6515 | Time: 115.23s
Train Loss: 0.6871 | Val Loss: 0.6435 | Time: 231.73s
Train Loss: 0.6770 | Val Loss: 0.6399 | Time: 366.25s
Train Loss: 0.6851 | Val Loss: 0.6561 | Time: 483.33s
Train Loss: 0.6846 | Val Loss: 0.6677 | Time: 605.56s
Train Loss: 0.6817 | Val Loss: 0.6636 | Time: 729.20s
Early stopping...

Epoch 3/4
Train Loss: 0.7111 | Val Loss: 0.6638 | Time: 120.72s
Early stopping...

Epoch 4/4
Train Loss: 0.6396 | Val Loss: 0.6581 | Time: 113.98s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.6874 | Val Loss: 0.7065 | Time: 133.29s
Train Loss: 0.6896 | Val Loss: 0.6620 | Time: 250.81s
Train Loss: 0.7046 | Val Loss: 0.6670 | Time: 385.77s
Train Loss: 0.6999 | Val Loss: 0.6541 | Time: 501.03s
Train Loss: 0.6971 | Val Loss: 0.6447 | Time: 620.62s
Train Loss: 0.6812 | Val Loss: 0.6362 | Time: 755.40s
Train Loss: 0.6805 | Val Loss: 0.6398 | Time: 868.92s
Train Loss: 0.6808 | Val Loss: 0.6309 | Time: 983.00s
Train Loss: 0.6797 | Val Loss: 0.6243 | Time: 1097.88s
Train Loss: 0.6860 | Val Loss: 0.6197 | Time: 1211.22s
Train Loss: 0.6856 | Val Loss: 0.6291 | Time: 1323.62s
Train Loss: 0.6783 | Val Loss: 0.6065 | Time: 1435.25s
Train Loss: 0.6761 | Val Loss: 0.5973 | Time: 1549.12s
Train Loss: 0.6835 | Val Loss: 0.6012 | Time: 1684.14s
Train Loss: 0.6820 | Val Loss: 0.6101 | Time: 1803.34s
Train Loss: 0.6896 | Val Loss: 0.6538 | Time: 1926.40s
Early stopping...

Epoch 2/2
Train Loss: 0.6506 | Val Loss: 0.6579 | Time: 121.35s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.7308 | Val Loss: 0.7734 | Time: 136.16s
Train Loss: 0.6549 | Val Loss: 0.7548 | Time: 271.97s
Train Loss: 0.7082 | Val Loss: 0.7288 | Time: 408.20s
Train Loss: 0.7118 | Val Loss: 0.7476 | Time: 572.32s
Train Loss: 0.7174 | Val Loss: 0.7336 | Time: 789.81s
Train Loss: 0.7041 | Val Loss: 0.7897 | Time: 1005.66s
Early stopping...

Epoch 2/3
Train Loss: 0.7570 | Val Loss: 0.6910 | Time: 219.35s
Train Loss: 0.7344 | Val Loss: 0.7133 | Time: 389.84s
Train Loss: 0.6985 | Val Loss: 0.6891 | Time: 558.22s
Train Loss: 0.7011 | Val Loss: 0.6438 | Time: 723.86s
Train Loss: 0.6812 | Val Loss: 0.6255 | Time: 939.49s
Train Loss: 0.6716 | Val Loss: 0.6180 | Time: 1110.40s
Train Loss: 0.6780 | Val Loss: 0.6274 | Time: 1326.42s
Train Loss: 0.6767 | Val Loss: 0.6207 | Time: 1492.21s
Train Loss: 0.6803 | Val Loss: 0.6164 | Time: 1665.45s
Train Loss: 0.6909 | Val Loss: 0.6153 | Time: 1835.74s
Train Loss: 0.6899 | Val Loss: 0.6199 | Time: 2003.11s
Train Loss: 0.6949 | Val Loss: 0.6236 | Time: 2175.56s
Train Loss: 0.6994 | Val Loss: 0.6289 | Time: 2346.17s
Early stopping...

Epoch 3/3
Train Loss: 0.6531 | Val Loss: 0.6291 | Time: 172.52s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.7924 | Val Loss: 0.6681 | Time: 216.56s
Train Loss: 0.7178 | Val Loss: 0.6527 | Time: 384.40s
Train Loss: 0.7107 | Val Loss: 0.6228 | Time: 558.19s
Train Loss: 0.7018 | Val Loss: 0.6374 | Time: 732.74s
Train Loss: 0.7033 | Val Loss: 0.6635 | Time: 908.78s
Train Loss: 0.7064 | Val Loss: 0.6682 | Time: 1080.23s
Early stopping...

Epoch 2/4
Train Loss: 0.6807 | Val Loss: 0.6765 | Time: 170.74s
Early stopping...

Epoch 3/4
Train Loss: 0.6973 | Val Loss: 0.6676 | Time: 170.89s
Early stopping...

Epoch 4/4
Train Loss: 0.6645 | Val Loss: 0.6384 | Time: 168.58s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.7035 | Val Loss: 0.6349 | Time: 240.56s
Train Loss: 0.6934 | Val Loss: 0.6393 | Time: 473.69s
Train Loss: 0.6953 | Val Loss: 0.6087 | Time: 704.06s
Train Loss: 0.6933 | Val Loss: 0.6873 | Time: 930.55s
Train Loss: 0.6968 | Val Loss: 0.7526 | Time: 1164.75s
Train Loss: 0.6895 | Val Loss: 0.6850 | Time: 1405.03s
Early stopping...

Epoch 2/2
Train Loss: 0.6374 | Val Loss: 0.7419 | Time: 236.87s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.6855 | Val Loss: 0.7419 | Time: 218.61s
Train Loss: 0.7062 | Val Loss: 0.7412 | Time: 424.11s
Train Loss: 0.6999 | Val Loss: 0.7500 | Time: 625.59s
Train Loss: 0.6982 | Val Loss: 0.6766 | Time: 830.61s
Train Loss: 0.6925 | Val Loss: 0.6528 | Time: 1039.02s
Train Loss: 0.6918 | Val Loss: 0.6913 | Time: 1258.40s
Train Loss: 0.6831 | Val Loss: 0.6309 | Time: 1539.73s
Train Loss: 0.6761 | Val Loss: 0.6161 | Time: 1820.23s
Train Loss: 0.6712 | Val Loss: 0.6139 | Time: 2119.45s
Train Loss: 0.6707 | Val Loss: 0.5504 | Time: 2398.48s
Train Loss: 0.6777 | Val Loss: 0.5428 | Time: 2692.84s
Train Loss: 0.6741 | Val Loss: 0.5351 | Time: 2981.71s
Train Loss: 0.6654 | Val Loss: 0.5244 | Time: 3277.04s
Train Loss: 0.6711 | Val Loss: 0.5308 | Time: 3560.49s
Train Loss: 0.6706 | Val Loss: 0.5379 | Time: 3866.27s
Train Loss: 0.6733 | Val Loss: 0.5356 | Time: 4150.40s
Early stopping...

Epoch 2/3
Train Loss: 0.6883 | Val Loss: 0.5390 | Time: 299.35s
Early stopping...

Epoch 3/3
Train Loss: 0.6331 | Val Loss: 0.5439 | Time: 283.44s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.7101 | Val Loss: 0.6025 | Time: 296.86s
Train Loss: 0.6945 | Val Loss: 0.6182 | Time: 581.14s
Train Loss: 0.6975 | Val Loss: 0.6888 | Time: 876.36s
Train Loss: 0.7005 | Val Loss: 0.6354 | Time: 1167.45s
Early stopping...

Epoch 2/4
Train Loss: 0.6753 | Val Loss: 0.6160 | Time: 300.24s
Early stopping...

Epoch 3/4
Train Loss: 0.6858 | Val Loss: 0.6240 | Time: 294.39s
Early stopping...

Epoch 4/4
Train Loss: 0.6401 | Val Loss: 0.6231 | Time: 314.39s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.6919 | Val Loss: 0.7513 | Time: 316.89s
Train Loss: 0.7074 | Val Loss: 0.7379 | Time: 643.61s
Train Loss: 0.7092 | Val Loss: 0.6816 | Time: 935.54s
Train Loss: 0.7057 | Val Loss: 0.6826 | Time: 1229.65s
Train Loss: 0.6958 | Val Loss: 0.6845 | Time: 1552.49s
Train Loss: 0.6955 | Val Loss: 0.6896 | Time: 1846.80s
Early stopping...

Epoch 2/2
Train Loss: 0.6454 | Val Loss: 0.6697 | Time: 309.50s
Train Loss: 0.6487 | Val Loss: 0.6821 | Time: 608.04s
Train Loss: 0.6455 | Val Loss: 0.6441 | Time: 903.88s
Train Loss: 0.6452 | Val Loss: 0.7018 | Time: 1203.30s
Train Loss: 0.6528 | Val Loss: 0.6330 | Time: 1487.35s
Train Loss: 0.6598 | Val Loss: 0.6347 | Time: 1785.95s
Train Loss: 0.6587 | Val Loss: 0.6240 | Time: 2091.94s
Train Loss: 0.6651 | Val Loss: 0.6173 | Time: 2403.23s
Train Loss: 0.6711 | Val Loss: 0.6170 | Time: 2710.78s
Train Loss: 0.6680 | Val Loss: 0.6433 | Time: 3017.96s
Train Loss: 0.6611 | Val Loss: 0.6478 | Time: 3329.78s
Train Loss: 0.6578 | Val Loss: 0.6213 | Time: 3640.32s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7215 | Val Loss: 0.6853 | Time: 307.44s
Train Loss: 0.7171 | Val Loss: 0.6445 | Time: 634.16s
Train Loss: 0.7067 | Val Loss: 0.6239 | Time: 927.43s
Train Loss: 0.6988 | Val Loss: 0.5740 | Time: 1213.48s
Train Loss: 0.6906 | Val Loss: 0.5515 | Time: 1536.62s
Train Loss: 0.6879 | Val Loss: 0.5470 | Time: 1855.06s
Train Loss: 0.6964 | Val Loss: 0.6014 | Time: 2152.13s
Train Loss: 0.6960 | Val Loss: 0.6721 | Time: 2455.61s
Train Loss: 0.6942 | Val Loss: 0.6987 | Time: 2760.77s
Early stopping...

Epoch 2/3
Train Loss: 0.7044 | Val Loss: 0.6935 | Time: 305.95s
Early stopping...

Epoch 3/3
Train Loss: 0.6828 | Val Loss: 0.6679 | Time: 295.91s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.7052 | Val Loss: 0.6482 | Time: 303.37s
Train Loss: 0.6936 | Val Loss: 0.5791 | Time: 615.79s
Train Loss: 0.6869 | Val Loss: 0.5723 | Time: 909.93s
Train Loss: 0.6799 | Val Loss: 0.5663 | Time: 1196.84s
Train Loss: 0.6816 | Val Loss: 0.5708 | Time: 1522.33s
Train Loss: 0.6800 | Val Loss: 0.5608 | Time: 1828.38s
Train Loss: 0.6792 | Val Loss: 0.5610 | Time: 2117.57s
Train Loss: 0.6791 | Val Loss: 0.5758 | Time: 2416.46s
Train Loss: 0.6791 | Val Loss: 0.6032 | Time: 2719.29s
Early stopping...

Epoch 2/4
Train Loss: 0.6414 | Val Loss: 0.5960 | Time: 299.46s
Early stopping...

Epoch 3/4
Train Loss: 0.6192 | Val Loss: 0.5985 | Time: 302.62s
Early stopping...

Epoch 4/4
Train Loss: 0.5545 | Val Loss: 0.5819 | Time: 308.86s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.7288 | Val Loss: 0.7012 | Time: 314.53s
Train Loss: 0.6994 | Val Loss: 0.6289 | Time: 641.87s
Train Loss: 0.6995 | Val Loss: 0.6314 | Time: 957.35s
Train Loss: 0.6955 | Val Loss: 0.6095 | Time: 1276.43s
Train Loss: 0.6930 | Val Loss: 0.6017 | Time: 1571.35s
Train Loss: 0.6905 | Val Loss: 0.5886 | Time: 1886.17s
Train Loss: 0.6859 | Val Loss: 0.5763 | Time: 2178.12s
Train Loss: 0.6842 | Val Loss: 0.5773 | Time: 2478.52s
Train Loss: 0.6862 | Val Loss: 0.5786 | Time: 2765.54s
Train Loss: 0.6781 | Val Loss: 0.5716 | Time: 3050.51s
Train Loss: 0.6802 | Val Loss: 0.5765 | Time: 3361.55s
Train Loss: 0.6860 | Val Loss: 0.5972 | Time: 3647.47s
Train Loss: 0.6860 | Val Loss: 0.6152 | Time: 3952.84s
Early stopping...

Epoch 2/2
Train Loss: 0.6516 | Val Loss: 0.6164 | Time: 291.24s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.7436 | Val Loss: 0.6519 | Time: 292.73s
Train Loss: 0.7051 | Val Loss: 0.6298 | Time: 578.18s
Train Loss: 0.7006 | Val Loss: 0.5961 | Time: 905.40s
Train Loss: 0.6934 | Val Loss: 0.6047 | Time: 1197.88s
Train Loss: 0.6931 | Val Loss: 0.5985 | Time: 1485.70s
Train Loss: 0.7036 | Val Loss: 0.6049 | Time: 1775.49s
Early stopping...

Epoch 2/3
Train Loss: 0.6583 | Val Loss: 0.5990 | Time: 284.56s
Early stopping...

Epoch 3/3
Train Loss: 0.7126 | Val Loss: 0.6086 | Time: 289.55s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.7188 | Val Loss: 0.6103 | Time: 296.69s
Train Loss: 0.7083 | Val Loss: 0.6573 | Time: 577.89s
Train Loss: 0.7171 | Val Loss: 0.6580 | Time: 872.05s
Train Loss: 0.7042 | Val Loss: 0.6119 | Time: 1162.92s
Early stopping...

Epoch 2/4
Train Loss: 0.7036 | Val Loss: 0.5828 | Time: 315.69s
Train Loss: 0.7004 | Val Loss: 0.5893 | Time: 616.76s
Train Loss: 0.6770 | Val Loss: 0.5613 | Time: 911.56s
Train Loss: 0.6704 | Val Loss: 0.5459 | Time: 1219.24s
Train Loss: 0.6696 | Val Loss: 0.5410 | Time: 1509.25s
Train Loss: 0.6592 | Val Loss: 0.5342 | Time: 1804.41s
Train Loss: 0.6572 | Val Loss: 0.5301 | Time: 2090.05s
Train Loss: 0.6640 | Val Loss: 0.5279 | Time: 2327.69s
Train Loss: 0.6661 | Val Loss: 0.5318 | Time: 2606.70s
Train Loss: 0.6681 | Val Loss: 0.5410 | Time: 2850.83s
Train Loss: 0.6668 | Val Loss: 0.5557 | Time: 3079.84s
Early stopping...

Epoch 3/4
Train Loss: 0.5858 | Val Loss: 0.5670 | Time: 238.23s
Early stopping...

Epoch 4/4
Train Loss: 0.6449 | Val Loss: 0.5768 | Time: 235.35s
Early stopping...

Best Params: batch=32, lr=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.6702 | Val Loss: 0.5503

Epoch 2/3
Train Loss: 0.6268 | Val Loss: 0.5535

Epoch 3/3
Train Loss: 0.5543 | Val Loss: 0.5552

Final Evaluation
Accuracy: 0.6212 | Precision: 0.6053 | Recall: 0.6970
F1 Score: 0.6479
Confusion Matrix:
[[18 15]
 [10 23]]
