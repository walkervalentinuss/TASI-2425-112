I0000 00:00:1746607239.914707      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5
I0000 00:00:1746607239.915396      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5
config.json: 100%
 1.54k/1.54k [00:00<00:00, 179kB/s]
tf_model.h5: 100%
 63.1M/63.1M [00:00<00:00, 214MB/s]
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Running experiment with batch_size=16, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.6861 | Val Loss: 0.7181 | Time: 6.23s
Train Loss: 0.7031 | Val Loss: 0.7079 | Time: 9.51s
Train Loss: 0.6926 | Val Loss: 0.7086 | Time: 12.71s
Train Loss: 0.6871 | Val Loss: 0.7211 | Time: 15.85s
Train Loss: 0.7051 | Val Loss: 0.7129 | Time: 18.94s
Early stopping...

Epoch 2/2
Train Loss: 0.7164 | Val Loss: 0.7023 | Time: 3.02s
Train Loss: 0.6736 | Val Loss: 0.6978 | Time: 6.02s
Train Loss: 0.7041 | Val Loss: 0.6950 | Time: 9.15s
Train Loss: 0.6923 | Val Loss: 0.6856 | Time: 12.16s
Train Loss: 0.6932 | Val Loss: 0.6837 | Time: 15.19s
Train Loss: 0.7036 | Val Loss: 0.6823 | Time: 18.16s
Train Loss: 0.7012 | Val Loss: 0.6800 | Time: 21.17s
Train Loss: 0.6996 | Val Loss: 0.6787 | Time: 24.17s
Train Loss: 0.7009 | Val Loss: 0.6807 | Time: 27.14s
Train Loss: 0.6978 | Val Loss: 0.6813 | Time: 30.14s
Train Loss: 0.7001 | Val Loss: 0.6829 | Time: 33.14s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/3
Train Loss: 0.6829 | Val Loss: 0.6971 | Time: 3.10s
Train Loss: 0.6781 | Val Loss: 0.6969 | Time: 6.21s
Train Loss: 0.7032 | Val Loss: 0.6871 | Time: 9.24s
Train Loss: 0.6991 | Val Loss: 0.6884 | Time: 12.26s
Train Loss: 0.6928 | Val Loss: 0.6897 | Time: 15.32s
Train Loss: 0.6910 | Val Loss: 0.6934 | Time: 18.35s
Early stopping...

Epoch 2/3
Train Loss: 0.6597 | Val Loss: 0.6956 | Time: 3.00s
Early stopping...

Epoch 3/3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.7337 | Val Loss: 0.7003 | Time: 2.99s
Early stopping...

Running experiment with batch_size=16, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.6936 | Val Loss: 0.6987 | Time: 3.12s
Train Loss: 0.7099 | Val Loss: 0.6998 | Time: 6.16s
Train Loss: 0.6881 | Val Loss: 0.7029 | Time: 9.16s
Train Loss: 0.6886 | Val Loss: 0.7066 | Time: 12.27s
Early stopping...

Epoch 2/4
Train Loss: 0.6708 | Val Loss: 0.7066 | Time: 3.03s
Early stopping...

Epoch 3/4
Train Loss: 0.6995 | Val Loss: 0.6957 | Time: 3.11s
Train Loss: 0.6862 | Val Loss: 0.6913 | Time: 6.29s
Train Loss: 0.6826 | Val Loss: 0.6891 | Time: 9.39s
Train Loss: 0.6681 | Val Loss: 0.6905 | Time: 12.45s
Train Loss: 0.6653 | Val Loss: 0.6963 | Time: 15.47s
Train Loss: 0.6623 | Val Loss: 0.7060 | Time: 18.51s
Early stopping...

Epoch 4/4
Train Loss: 0.5918 | Val Loss: 0.7154 | Time: 3.08s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.

Epoch 1/2
Train Loss: 0.6811 | Val Loss: 0.7034 | Time: 3.17s
Train Loss: 0.6878 | Val Loss: 0.7017 | Time: 6.30s
Train Loss: 0.6962 | Val Loss: 0.6954 | Time: 9.37s
Train Loss: 0.7106 | Val Loss: 0.6945 | Time: 12.57s
Train Loss: 0.7160 | Val Loss: 0.6998 | Time: 15.70s
Train Loss: 0.7120 | Val Loss: 0.6959 | Time: 18.73s
Train Loss: 0.7043 | Val Loss: 0.6957 | Time: 21.76s
Early stopping...

Epoch 2/2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6602 | Val Loss: 0.6958 | Time: 3.02s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.8099 | Val Loss: 0.7054 | Time: 3.09s
Train Loss: 0.7797 | Val Loss: 0.7031 | Time: 6.07s
Train Loss: 0.7524 | Val Loss: 0.7088 | Time: 9.12s
Train Loss: 0.7332 | Val Loss: 0.7105 | Time: 12.13s
Train Loss: 0.7269 | Val Loss: 0.7131 | Time: 15.24s
Early stopping...

Epoch 2/3
Train Loss: 0.7166 | Val Loss: 0.7105 | Time: 3.04s
Early stopping...

Epoch 3/3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.7667 | Val Loss: 0.7033 | Time: 3.09s
Early stopping...

Running experiment with batch_size=16, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.6706 | Val Loss: 0.6877 | Time: 3.11s
Train Loss: 0.6883 | Val Loss: 0.6857 | Time: 6.12s
Train Loss: 0.7042 | Val Loss: 0.6898 | Time: 9.11s
Train Loss: 0.7028 | Val Loss: 0.6887 | Time: 12.10s
Train Loss: 0.6917 | Val Loss: 0.6913 | Time: 15.12s
Early stopping...

Epoch 2/4
Train Loss: 0.6826 | Val Loss: 0.6915 | Time: 3.05s
Early stopping...

Epoch 3/4
Train Loss: 0.7115 | Val Loss: 0.6898 | Time: 3.03s
Early stopping...

Epoch 4/4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.7148 | Val Loss: 0.6892 | Time: 3.06s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.7037 | Val Loss: 0.7084 | Time: 3.09s
Train Loss: 0.7529 | Val Loss: 0.7020 | Time: 6.08s
Train Loss: 0.7292 | Val Loss: 0.6971 | Time: 9.20s
Train Loss: 0.7108 | Val Loss: 0.6965 | Time: 12.21s
Train Loss: 0.7095 | Val Loss: 0.6957 | Time: 15.24s
Train Loss: 0.7146 | Val Loss: 0.6937 | Time: 18.22s
Train Loss: 0.7115 | Val Loss: 0.6913 | Time: 21.21s
Train Loss: 0.6993 | Val Loss: 0.6907 | Time: 24.24s
Train Loss: 0.6973 | Val Loss: 0.6903 | Time: 27.24s
Train Loss: 0.6945 | Val Loss: 0.6919 | Time: 30.29s
Train Loss: 0.6954 | Val Loss: 0.6922 | Time: 33.35s
Train Loss: 0.6945 | Val Loss: 0.6925 | Time: 36.37s
Early stopping...

Epoch 2/2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6880 | Val Loss: 0.6944 | Time: 3.02s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.6608 | Val Loss: 0.7115 | Time: 3.11s
Train Loss: 0.6920 | Val Loss: 0.7058 | Time: 6.18s
Train Loss: 0.6801 | Val Loss: 0.6998 | Time: 9.19s
Train Loss: 0.6859 | Val Loss: 0.7038 | Time: 12.24s
Train Loss: 0.6679 | Val Loss: 0.7048 | Time: 15.27s
Train Loss: 0.6813 | Val Loss: 0.7028 | Time: 18.28s
Early stopping...

Epoch 2/3
Train Loss: 0.7386 | Val Loss: 0.6975 | Time: 3.10s
Train Loss: 0.7152 | Val Loss: 0.6963 | Time: 6.18s
Train Loss: 0.7106 | Val Loss: 0.7003 | Time: 9.20s
Train Loss: 0.7122 | Val Loss: 0.7012 | Time: 12.18s
Train Loss: 0.7188 | Val Loss: 0.7018 | Time: 15.19s
Early stopping...

Epoch 3/3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6924 | Val Loss: 0.7049 | Time: 3.03s
Early stopping...

Running experiment with batch_size=16, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.7650 | Val Loss: 0.7031 | Time: 3.09s
Train Loss: 0.7221 | Val Loss: 0.7006 | Time: 6.07s
Train Loss: 0.7020 | Val Loss: 0.7035 | Time: 9.07s
Train Loss: 0.7075 | Val Loss: 0.6986 | Time: 12.07s
Train Loss: 0.6989 | Val Loss: 0.6952 | Time: 15.04s
Train Loss: 0.7020 | Val Loss: 0.6977 | Time: 18.16s
Train Loss: 0.6947 | Val Loss: 0.6983 | Time: 21.15s
Train Loss: 0.6903 | Val Loss: 0.6971 | Time: 24.17s
Early stopping...

Epoch 2/4
Train Loss: 0.7169 | Val Loss: 0.6963 | Time: 3.03s
Early stopping...

Epoch 3/4
Train Loss: 0.6826 | Val Loss: 0.6955 | Time: 3.03s
Early stopping...

Epoch 4/4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6655 | Val Loss: 0.6966 | Time: 3.02s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=2

Epoch 1/2
Train Loss: 0.7388 | Val Loss: 0.7257 | Time: 3.08s
Train Loss: 0.7268 | Val Loss: 0.7268 | Time: 6.05s
Train Loss: 0.7051 | Val Loss: 0.7231 | Time: 9.00s
Train Loss: 0.7195 | Val Loss: 0.7012 | Time: 11.95s
Train Loss: 0.7140 | Val Loss: 0.6786 | Time: 14.92s
Train Loss: 0.7094 | Val Loss: 0.6784 | Time: 17.86s
Train Loss: 0.7082 | Val Loss: 0.6775 | Time: 20.81s
Train Loss: 0.7041 | Val Loss: 0.6842 | Time: 23.76s
Train Loss: 0.7041 | Val Loss: 0.6845 | Time: 26.70s
Train Loss: 0.7024 | Val Loss: 0.6935 | Time: 29.65s
Early stopping...

Epoch 2/2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6850 | Val Loss: 0.6938 | Time: 2.94s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=3

Epoch 1/3
Train Loss: 0.7034 | Val Loss: 0.7088 | Time: 3.05s
Train Loss: 0.6918 | Val Loss: 0.7039 | Time: 6.02s
Train Loss: 0.6883 | Val Loss: 0.6996 | Time: 8.98s
Train Loss: 0.6959 | Val Loss: 0.6936 | Time: 11.95s
Train Loss: 0.6991 | Val Loss: 0.6981 | Time: 14.93s
Train Loss: 0.6981 | Val Loss: 0.6969 | Time: 17.89s
Train Loss: 0.6979 | Val Loss: 0.6983 | Time: 20.84s
Early stopping...

Epoch 2/3
Train Loss: 0.6921 | Val Loss: 0.6951 | Time: 2.96s
Early stopping...

Epoch 3/3
Train Loss: 0.6881 | Val Loss: 0.6915 | Time: 2.96s
Train Loss: 0.6849 | Val Loss: 0.6878 | Time: 5.91s
Train Loss: 0.6915 | Val Loss: 0.6856 | Time: 8.88s
Train Loss: 0.6937 | Val Loss: 0.6875 | Time: 11.83s
Train Loss: 0.6935 | Val Loss: 0.6873 | Time: 14.80s
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6944 | Val Loss: 0.6871 | Time: 17.76s
Early stopping...

Running experiment with batch_size=32, learning_rate=5e-05, epochs=4

Epoch 1/4
Train Loss: 0.7055 | Val Loss: 0.7156 | Time: 3.05s
Train Loss: 0.7048 | Val Loss: 0.7107 | Time: 6.02s
Train Loss: 0.7156 | Val Loss: 0.7069 | Time: 8.99s
Train Loss: 0.7096 | Val Loss: 0.7012 | Time: 11.96s
Train Loss: 0.7021 | Val Loss: 0.6967 | Time: 14.92s
Train Loss: 0.6990 | Val Loss: 0.7038 | Time: 17.89s
Train Loss: 0.6960 | Val Loss: 0.7044 | Time: 20.88s
Train Loss: 0.6937 | Val Loss: 0.7049 | Time: 23.85s
Early stopping...

Epoch 2/4
Train Loss: 0.6697 | Val Loss: 0.7118 | Time: 2.96s
Early stopping...

Epoch 3/4
Train Loss: 0.6482 | Val Loss: 0.7122 | Time: 2.97s
Early stopping...

Epoch 4/4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.7736 | Val Loss: 0.7076 | Time: 2.97s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=2

Epoch 1/2
Train Loss: 0.7464 | Val Loss: 0.7046 | Time: 3.06s
Train Loss: 0.7109 | Val Loss: 0.7017 | Time: 6.03s
Train Loss: 0.6975 | Val Loss: 0.7028 | Time: 8.99s
Train Loss: 0.7072 | Val Loss: 0.6988 | Time: 11.94s
Train Loss: 0.7128 | Val Loss: 0.6951 | Time: 14.90s
Train Loss: 0.7143 | Val Loss: 0.6914 | Time: 17.86s
Train Loss: 0.7123 | Val Loss: 0.6886 | Time: 20.82s
Train Loss: 0.7086 | Val Loss: 0.6913 | Time: 23.78s
Train Loss: 0.7052 | Val Loss: 0.6902 | Time: 26.74s
Train Loss: 0.7055 | Val Loss: 0.6870 | Time: 29.70s
Train Loss: 0.7038 | Val Loss: 0.6879 | Time: 32.67s
Train Loss: 0.7020 | Val Loss: 0.6886 | Time: 35.64s
Train Loss: 0.7011 | Val Loss: 0.6902 | Time: 38.61s
Early stopping...

Epoch 2/2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6918 | Val Loss: 0.6889 | Time: 2.95s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=3

Epoch 1/3
Train Loss: 0.7014 | Val Loss: 0.6973 | Time: 3.04s
Train Loss: 0.6902 | Val Loss: 0.7034 | Time: 6.01s
Train Loss: 0.6944 | Val Loss: 0.6955 | Time: 8.96s
Train Loss: 0.6993 | Val Loss: 0.6930 | Time: 11.91s
Train Loss: 0.7025 | Val Loss: 0.6877 | Time: 14.85s
Train Loss: 0.7027 | Val Loss: 0.6828 | Time: 17.80s
Train Loss: 0.6967 | Val Loss: 0.6847 | Time: 20.74s
Train Loss: 0.6990 | Val Loss: 0.6851 | Time: 23.69s
Train Loss: 0.6999 | Val Loss: 0.6882 | Time: 26.64s
Early stopping...

Epoch 2/3
Train Loss: 0.6506 | Val Loss: 0.6893 | Time: 2.94s
Early stopping...

Epoch 3/3
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6228 | Val Loss: 0.6915 | Time: 2.94s
Early stopping...

Running experiment with batch_size=32, learning_rate=3e-05, epochs=4

Epoch 1/4
Train Loss: 0.6989 | Val Loss: 0.6993 | Time: 3.04s
Train Loss: 0.7121 | Val Loss: 0.6855 | Time: 5.99s
Train Loss: 0.7044 | Val Loss: 0.6886 | Time: 8.94s
Train Loss: 0.7017 | Val Loss: 0.6943 | Time: 11.89s
Train Loss: 0.7012 | Val Loss: 0.6943 | Time: 14.84s
Early stopping...

Epoch 2/4
Train Loss: 0.6619 | Val Loss: 0.6984 | Time: 2.96s
Early stopping...

Epoch 3/4
Train Loss: 0.6907 | Val Loss: 0.6981 | Time: 2.95s
Early stopping...

Epoch 4/4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6906 | Val Loss: 0.6967 | Time: 2.99s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=2

Epoch 1/2
Train Loss: 0.6893 | Val Loss: 0.7018 | Time: 3.04s
Train Loss: 0.6963 | Val Loss: 0.7130 | Time: 6.00s
Train Loss: 0.6947 | Val Loss: 0.7109 | Time: 8.96s
Train Loss: 0.7027 | Val Loss: 0.7067 | Time: 11.92s
Early stopping...

Epoch 2/2
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.7313 | Val Loss: 0.7032 | Time: 2.96s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.6857 | Val Loss: 0.6983 | Time: 3.03s
Train Loss: 0.7092 | Val Loss: 0.6922 | Time: 5.97s
Train Loss: 0.7099 | Val Loss: 0.6890 | Time: 8.91s
Train Loss: 0.7056 | Val Loss: 0.6876 | Time: 11.86s
Train Loss: 0.6993 | Val Loss: 0.6855 | Time: 14.81s
Train Loss: 0.6978 | Val Loss: 0.6830 | Time: 17.76s
Train Loss: 0.6973 | Val Loss: 0.6830 | Time: 20.70s
Train Loss: 0.6988 | Val Loss: 0.6855 | Time: 23.65s
Train Loss: 0.6990 | Val Loss: 0.6852 | Time: 26.59s
Early stopping...

Epoch 2/3
Train Loss: 0.6557 | Val Loss: 0.6832 | Time: 2.94s
Early stopping...

Epoch 3/3
Train Loss: 0.6776 | Val Loss: 0.6826 | Time: 2.95s
Train Loss: 0.6792 | Val Loss: 0.6816 | Time: 5.89s
Train Loss: 0.6725 | Val Loss: 0.6813 | Time: 8.84s
Train Loss: 0.6532 | Val Loss: 0.6786 | Time: 11.79s
Train Loss: 0.6585 | Val Loss: 0.6762 | Time: 14.74s
Train Loss: 0.6640 | Val Loss: 0.6735 | Time: 17.68s
Train Loss: 0.6639 | Val Loss: 0.6732 | Time: 20.64s
Train Loss: 0.6625 | Val Loss: 0.6751 | Time: 23.59s
Train Loss: 0.6617 | Val Loss: 0.6767 | Time: 26.55s
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6592 | Val Loss: 0.6760 | Time: 29.51s
Early stopping...

Running experiment with batch_size=32, learning_rate=2e-05, epochs=4

Epoch 1/4
Train Loss: 0.7137 | Val Loss: 0.6996 | Time: 3.47s
Train Loss: 0.7111 | Val Loss: 0.6994 | Time: 6.45s
Train Loss: 0.7026 | Val Loss: 0.6987 | Time: 9.41s
Train Loss: 0.7060 | Val Loss: 0.7011 | Time: 12.37s
Train Loss: 0.7010 | Val Loss: 0.7026 | Time: 15.38s
Train Loss: 0.7023 | Val Loss: 0.7037 | Time: 18.37s
Early stopping...

Epoch 2/4
Train Loss: 0.7283 | Val Loss: 0.7043 | Time: 2.97s
Early stopping...

Epoch 3/4
Train Loss: 0.6864 | Val Loss: 0.7040 | Time: 2.98s
Early stopping...

Epoch 4/4
Some layers from the model checkpoint at indobenchmark/indobert-lite-base-p1 were not used when initializing TFAlbertForSequenceClassification: ['sop_classifier']
- This IS expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing TFAlbertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some layers of TFAlbertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-lite-base-p1 and are newly initialized: ['classifier']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Train Loss: 0.6121 | Val Loss: 0.7027 | Time: 2.96s
Early stopping...

Best Params: batch=32, lr=2e-05, epochs=3

Epoch 1/3
Train Loss: 0.6771 | Val Loss: 0.6428

Epoch 2/3
Train Loss: 0.6072 | Val Loss: 0.6224

Epoch 3/3
Train Loss: 0.5574 | Val Loss: 0.6024

Final Evaluation
Accuracy: 0.6955 | Precision: 0.7590 | Recall: 0.5727
F1 Score: 0.6528
Confusion Matrix:
[[90 20]
 [47 63]]